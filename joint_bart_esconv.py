# -*- coding: utf-8 -*-
"""
joint_bart_esconv.py
====================
ESConv ì „ëµ ì˜ˆì¸¡ + ì‘ë‹µ ìƒì„± Joint-Decoding íŒŒì´í”„ë¼ì¸

í•µì‹¬ íŠ¹ì§•
----------
1. **ë°ì´í„°** : HuggingFace `thu-coai/esconv` â†’ ì‹œìŠ¤í…œ í„´ë§ˆë‹¤ <BOS> <STRAT_xxx> ì‘ë‹µ ì‹œí€€ìŠ¤ êµ¬ì„±.
2. **ëª¨ë¸** : BART-base + classification head, ê³µë™ loss `loss_gen + Î»Â·loss_cls`.
3. **ì¶”ë¡ ** : enc-CLS â†’ strategy logits â†’ í† í° prefix ë‘ê³  `model.generate()`.
4. **ë©€í‹°GPU** : `torchrun` ê·¸ëŒ€ë¡œ ì‚¬ìš© (Trainer + DDP).
5. **ë©”íŠ¸ë¦­** : ì „ëµ acc/f1, ì‘ë‹µ BLEU1-4Â·ROUGE-LÂ·METEORÂ·CIDErÂ·PPL.
6. **ë‹¨ì¼ íŒŒì´ì¬ íŒŒì¼** ë¡œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥.

ì‹¤í–‰ ì˜ˆì‹œ
----------
# 1 GPU
CUDA_VISIBLE_DEVICES=0 python joint_bart_esconv.py \
    --epochs 10 --batch_size 16 --lambda_cls 0.5 \
    --clean_checkpoints --eval_init \
    --output_dir outputs/joint

# 1GPU, tiny check
CUDA_VISIBLE_DEVICES=3 python joint_bart_esconv.py \
        --epochs 3 --batch_size 4 --tiny_frac 0.01 --lambda_cls 0.5 \
        --clean_checkpoints --eval_init --output_dir outputs/sanity1gpu
"""
from __future__ import annotations

import os, sys, argparse, json, logging, random, math
from datetime import datetime
from pathlib import Path
from collections import defaultdict

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm.auto import tqdm

from transformers import (
    BartTokenizer,
    BartForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    set_seed,
    GenerationConfig,
    EarlyStoppingCallback,
    TrainerCallback,
)

# ============================== global config & logging ==============================
SPECIAL_TOKENS = {
    "usr": "[USR]",
    "sys": "[SYS]",
    "sep": "[SEP]",
    "hist": "[STRAT_HIST]",
    "strategy": "[STRATEGY]",
    "strategy_placeholder": "[STRATEGY_EMBEDDING]",  # NEW: dynamic strategy embedding placeholder
}
STRATEGIES = [
    "Question",
    "Restatement or Paraphrasing",
    "Reflection of feelings",
    "Self-disclosure",
    "Affirmation and Reassurance",
    "Providing Suggestions",
    "Information",
    "Others",
]
STR2ID = {s: i for i, s in enumerate(STRATEGIES)}
ID2STR = {i: s for s, i in STR2ID.items()}
STRAT_TOKENS = [f"[STRAT_{s.replace(' ', '_')}]" for s in STRATEGIES]

logger = logging.getLogger("joint")

# ============================== dataset ==============================================
class JointESConvDataset(torch.utils.data.Dataset):
    """ì‹œìŠ¤í…œ í„´ ë‹¨ìœ„ ì˜ˆì œ ìƒì„± + joint decoding í¬ë§·"""

    def __init__(self, split: str, tokenizer: BartTokenizer, max_src: int = 512, max_tgt: int = 64, cache_dir: str = "cache_joint"):
        self.tok = tokenizer
        self.max_src = max_src
        self.max_tgt = max_tgt
        raw = load_dataset("thu-coai/esconv")[split]

        cache_file = Path(cache_dir) / f"{split}.pt"
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        if cache_file.exists():
            self.examples = torch.load(cache_file, weights_only=True, map_location="cpu")
            return

        examples = []
        for ex in tqdm(raw, desc=f"proc {split}"):
            dialog = json.loads(ex["text"])["dialog"]
            sys_turns = [(i, t) for i, t in enumerate(dialog) if t["speaker"] == "sys"]
            for idx, (turn_idx, turn) in enumerate(sys_turns):
                strat = turn.get("strategy", "Others")
                strat_id = STR2ID.get(strat, STR2ID["Others"])

                # context : ëª¨ë“  ì´ì „ turn + ì—­í•  í† í°
                ctx_parts = []
                for t in dialog[:turn_idx]:
                    spk_tok = SPECIAL_TOKENS["usr"] if t["speaker"] == "usr" else SPECIAL_TOKENS["sys"]
                    if t["speaker"] == "sys":
                        st = t.get("strategy", "Others")
                        st_tok = f"[STRAT_{st.replace(' ', '_')}]"
                        ctx_parts.append(f"{SPECIAL_TOKENS['strategy']} {st_tok} {spk_tok} {t['text']}")
                    else:
                        ctx_parts.append(f"{spk_tok} {t['text']}")
                context = " ".join(ctx_parts)

                # decoder input = BOS STRAT_TOKEN response
                d_in = f"{SPECIAL_TOKENS['strategy']} {SPECIAL_TOKENS['strategy_placeholder']} {SPECIAL_TOKENS['sys']} {turn['text']}"

                examples.append({
                    "context": context,
                    "decoder_text": d_in,
                    "response": turn["text"],
                    "strategy_id": strat_id,
                })
        self.examples = examples
        torch.save(self.examples, cache_file)

        if len(self.examples) > 0:
            print("\n===== ë°ì´í„°ì…‹ ì˜ˆì œ ìƒ˜í”Œ =====")
            print(f"ì´ ì˜ˆì œ ìˆ˜: {len(self.examples)}")
            sample_idx = min(len(self.examples)-1, 5)  # 5ë²ˆì§¸ ì˜ˆì œ ë˜ëŠ” ë§ˆì§€ë§‰ ì˜ˆì œ
            ex = self.examples[sample_idx]
            print(f"Context: {ex['context']}...")  # ì•ë¶€ë¶„ 100ìë§Œ
            print(f"Decoder Text: {ex['decoder_text']}")
            print(f"Response: {ex['response']}")
            print(f"Strategy ID: {ex['strategy_id']} ({ID2STR[ex['strategy_id']]})")
            print("============================\n")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        ex = self.examples[idx]
        enc = self.tok(
            ex["context"], max_length=self.max_src, truncation=True, padding="max_length", return_tensors="pt"
        )
        dec = self.tok(
            ex["decoder_text"], max_length=self.max_tgt, truncation=True, padding="max_length", add_special_tokens=False, return_tensors="pt"
        )
        seq = dec["input_ids"].squeeze()             # length <= max_tgt
        seq = seq[: self.max_tgt - 1]                 # reserve 1 slot for EOS
        labels = torch.cat([seq, torch.tensor([self.tok.eos_token_id])])
        # pad to max_tgt
        if labels.size(0) < self.max_tgt:
            pad_len = self.max_tgt - labels.size(0)
            pad = torch.full((pad_len,), self.tok.pad_token_id, dtype=torch.long)
            labels = torch.cat([labels, pad])

        # mask loss on pad
        labels_masked = labels.clone()
        labels_masked[labels_masked == self.tok.pad_token_id] = -100

        # ì²« ë²ˆì§¸ í† í°([STRATEGY])ì€ ë‹¨ìˆœ ë§ˆì»¤ì´ë¯€ë¡œ loss ê³„ì‚°ì—ì„œ ì œì™¸
        labels_masked[0] = -100
        # ë‘ ë²ˆì§¸ í† í°([STRATEGY_EMBEDDING])ì€ ëª¨ë¸ì´ ì§ì ‘ ì„ë² ë”©ì„ ì£¼ì…í•˜ë¯€ë¡œ loss ê³„ì‚°ì—ì„œ ì œì™¸
        labels_masked[1] = -100
        # ì„¸ ë²ˆì§¸ í† í°([SYS])ë„ ë‹¨ìˆœ ë§ˆì»¤ì´ë¯€ë¡œ loss ê³„ì‚°ì—ì„œ ì œì™¸
        labels_masked[2] = -100

        decoder_input_ids = torch.cat([
            torch.tensor([self.tok.bos_token_id]),
            labels[:-1]  # everything except last token (EOS or PAD)
        ])

        return {
            "input_ids": enc["input_ids"].squeeze(),
            "attention_mask": enc["attention_mask"].squeeze(),
            "decoder_input_ids": decoder_input_ids,
            "labels": labels_masked,
            "strategy_id": torch.tensor(ex["strategy_id"], dtype=torch.long),
        }

# ============================== model ===============================================
class JointBart(nn.Module):
    def __init__(self, tokenizer: BartTokenizer, lambda_cls: float = 0.5):
        super().__init__()
        self.tok = tokenizer
        self.lambda_cls = lambda_cls
        self.model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
        self.model.resize_token_embeddings(len(tokenizer))
        self.classifier = nn.Linear(self.model.config.d_model, len(STRATEGIES))
        # h_cls â†’ ë””ì½”ë”ë¡œ ì „ë‹¬ë  ì„ë² ë”© íˆ¬ì‚¬ìš© projection
        self.strat_proj = nn.Linear(self.model.config.d_model, self.model.config.d_model, bias=False)
        nn.init.normal_(self.classifier.weight, std=0.02)
        self.model.generation_config.forced_bos_token_id = None
        self.model.generation_config.decoder_start_token_id = None   # prefixë¡œ BOS ë„£ì—ˆìœ¼ë¯€ë¡œ ì•ˆì „ì¥ì¹˜

        # Seq2SeqTrainer ë‚´ë¶€ ë¡œì§ì€ self.model(generation_config)ì„ ì§ì ‘ ì°¸ì¡°í•˜ë¯€ë¡œ
        # wrapper í´ë˜ìŠ¤ì—ë„ ë™ì¼ ì†ì„±ì„ ë…¸ì¶œí•´ ì¤€ë‹¤.
        self.generation_config = self.model.generation_config

        # HF Trainerê°€ ë¡œë“œ/ì„¸ì´ë¸Œ ê³¼ì •ì—ì„œ ì°¸ì¡°í•˜ëŠ” ì•ˆì „ìš© attribute ì¶”ê°€
        self._keys_to_ignore_on_save: list[str] | None = []
        self._keys_to_ignore_on_load_missing: list[str] | None = []
        self._keys_to_ignore_on_load_unexpected: list[str] | None = []

    def forward(
        self,
        input_ids,
        attention_mask,
        decoder_input_ids=None,
        labels=None,
        strategy_id=None,
    ):
        """Joint forward with dynamic strategy embedding injection."""
        # ---------------- 1) Encoder -----------------
        enc_outputs = self.model.model.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True,
        )
        h_cls = enc_outputs.last_hidden_state[:, 0]  # (B, d_model)

        # strategy classification
        strat_logits = self.classifier(h_cls)  # (B, |S|)

        # ---------------- 2') Classification only ê²½ë¡œ -----------------
        # evaluation ë£¨í‹´ ì¤‘ strategy accuracy í‰ê°€ì²˜ëŸ¼, decoder_input_ids ì—†ì´
        # encoder CLS ê¸°ë°˜ ë¶„ë¥˜ ì ìˆ˜ë§Œ í•„ìš”í•  ë•Œê°€ ìˆë‹¤. ì´ ê²½ìš° ë””ì½”ë” ë°
        # ìƒì„± loss ê³„ì‚°ì„ ìƒëµí•˜ê³  strategy_logits ë§Œ ë°˜í™˜í•œë‹¤.

        if decoder_input_ids is None:
            from types import SimpleNamespace

            # ì†ì‰¬ìš´ ë°˜í™˜ ê°ì²´ ìƒì„± (loss=None, encoder_last_hidden_state í¬í•¨)
            return SimpleNamespace(
                loss=None,
                strategy_logits=strat_logits,
                encoder_last_hidden_state=enc_outputs.last_hidden_state,
            )

        # ---------------- 3) Decoder embedding êµ¬ì„± -----------------
        # token id â†’ embedding
        # BART ëª¨ë¸ì€ ì¸ì½”ë”ì™€ ë””ì½”ë”ê°€ ë™ì¼í•œ í† í° ì„ë² ë”©(nn.Embedding)ì„ ê³µìœ 
        # ê³µìœ  ì„ë² ë”© ë ˆì´ì–´ê°€ ë°”ë¡œ model.shared
        dec_emb = self.model.model.shared(decoder_input_ids)  # (B, L, d_model)

        # placeholder ìœ„ì¹˜ëŠ” í•­ìƒ index 1 (BOS ë’¤)
        dec_emb[:, 1, :] = self.strat_proj(h_cls)

        # ---------------- 4) ì „ì²´ ëª¨ë¸ forward -----------------
        outputs = self.model(
            input_ids=None,  # encoder_outputs ë¡œ ëŒ€ì²´
            attention_mask=attention_mask,
            encoder_outputs=enc_outputs,
            decoder_inputs_embeds=dec_emb,
            labels=labels,
            return_dict=True,
        )

        # ---------------- 5) Loss í•©ì‚° -----------------
        loss = outputs.loss
        if strategy_id is not None:
            cls_loss = F.cross_entropy(strat_logits, strategy_id)
            loss = loss + self.lambda_cls * cls_loss

        outputs.loss = loss
        outputs.strategy_logits = strat_logits
        return outputs

    # joint decoding
    def generate(self, input_ids, attention_mask, **gen_kwargs):
        with torch.no_grad():
            cls_hidden = self.model.model.encoder(
                input_ids=input_ids, attention_mask=attention_mask, return_dict=True
            ).last_hidden_state[:, 0]

            # ì „ëµ ë¡œì§“ì€ ì—¬ì „íˆ ê³„ì‚°í•˜ë˜, ë””ì½”ë” prefixëŠ” ê³ ì • placeholder ì‚¬ìš©
            strat_logits = self.classifier(cls_hidden)

            placeholder_id = self.tok.convert_tokens_to_ids(SPECIAL_TOKENS["strategy_placeholder"])
            batch_size = input_ids.size(0)
            prefix = torch.stack(
                [torch.tensor([self.tok.bos_token_id, placeholder_id]) for _ in range(batch_size)]
            ).to(input_ids.device)

            # -------- ë””ì½”ë”© í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ --------
            # # (1) ë¬¸ì¥ ê¸¸ì´ ì œí•œ: 10 â‰¤ length â‰¤ 50
            # gen_kwargs.setdefault("max_new_tokens", 50)
            # gen_kwargs.setdefault("min_length", 10)

            # (2) ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° â€“ ìì—°ìŠ¤ëŸ¬ì›€ & ë‹¤ì–‘ì„±
            gen_kwargs.setdefault("do_sample", False)
            gen_kwargs.setdefault("no_repeat_ngram_size", 3)
            gen_kwargs.setdefault("repetition_penalty", 1.2)
            gen_kwargs.setdefault("length_penalty", 1.2)
            gen_kwargs.setdefault("num_beams", 5)
            gen_kwargs.setdefault("early_stopping", True)
            gen_kwargs.setdefault("eos_token_id", self.tok.eos_token_id)
             
            # íŠ¸ë ˆì´ë„ˆê°€ ì „ë‹¬í•œ generation_max_length(=max_length) ê°’ì„ ì œê±°í•˜ì—¬
            # "Both max_new_tokens and max_length" ê²½ê³  ë©”ì‹œì§€ë¥¼ ë°©ì§€í•œë‹¤.
            if "max_new_tokens" in gen_kwargs and "max_length" in gen_kwargs:
                gen_kwargs.pop("max_length")

            return self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                decoder_input_ids=prefix,
                **gen_kwargs,
            )

# ============================== metric utils ========================================
from pycocoevalcap.bleu.bleu import Bleu
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.cider.cider import Cider

bleu_scorer = Bleu(4)
rouge_scorer = Rouge()
meteor_scorer = Meteor()
cider_scorer = Cider()


def generation_metrics(preds: list[str], refs: list[str]):
    gts = {i: [r] for i, r in enumerate(refs)}
    res = {i: [p] for i, p in enumerate(preds)}
    bleu, _ = bleu_scorer.compute_score(gts, res)
    rouge, _ = rouge_scorer.compute_score(gts, res)
    meteor, _ = meteor_scorer.compute_score(gts, res)
    cider, _ = cider_scorer.compute_score(gts, res)
    return {
        "bleu1": bleu[0],
        "bleu2": bleu[1],
        "bleu3": bleu[2],
        "bleu4": bleu[3],
        "rouge_l": rouge,
        "meteor": meteor,
        "cider": cider,
    }


def calculate_perplexity(loss):
    """
    ì†ì‹¤ê°’(cross-entropy loss)ì—ì„œ perplexity ê³„ì‚°
    PPL = exp(í‰ê·  negative log likelihood) = exp(loss)
    """
    return float(np.exp(loss))

# ============================== trainer ==============================================
class JointTrainer(Seq2SeqTrainer):
    # ------------------------------------------------------------------
    # Seq2SeqTrainer ëŠ” tokenizer ì¸ìë¥¼ deprecated ì²˜ë¦¬í•˜ë¯€ë¡œ, ì „ë‹¬ëœ tokenizer
    #               â†’ processing_class ë¡œ ì˜®ê²¨ì„œ ê²½ê³ ë¥¼ ì ì¬ìš´ë‹¤.
    # ------------------------------------------------------------------
    def __init__(self, *args, **kwargs):
        tok = kwargs.pop("tokenizer", None)
        if tok is not None and "processing_class" not in kwargs:
            kwargs["processing_class"] = tok  # ì‹ ë²„ì „ Trainer ê°€ ê¶Œì¥í•˜ëŠ” í•„ë“œ
        
        # í´ë˜ìŠ¤ ë©”ì„œë“œë¥¼ compute_metricsë¡œ ì„¤ì • (ì¤‘ìš”: ê¸°ì¡´ì— ì „ë‹¬ëœ ê²ƒì´ ì—†ì„ ë•Œë§Œ)
        if 'compute_metrics' not in kwargs or kwargs['compute_metrics'] is None:
            kwargs['compute_metrics'] = self.compute_metrics
        
        # Best ëª¨ë¸ ì¶”ì  ë³€ìˆ˜
        self.best_metric = float("inf")  # eval_lossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        self.best_model_dir = os.path.join(kwargs.get("args").output_dir, "best_model")
        os.makedirs(self.best_model_dir, exist_ok=True)
        self.metric_for_best_model = kwargs.get("args").metric_for_best_model
        self.greater_is_better = kwargs.get("args").greater_is_better
        
        import logging
        self.logger = logging.getLogger("joint")
        
        super().__init__(*args, **kwargs)
        
        # metric ê³„ì‚°ìš© ìºì‹œ ì´ˆê¸°í™”
        self.strategy_logits_cache: list[np.ndarray] = []
        self.sid_cache: list[np.ndarray] = []

    def compute_metrics(self, eval_pred):
        """í…ìŠ¤íŠ¸ ìƒì„± ë° ì „ëµ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        import logging
        logger = logging.getLogger("joint")
        
        preds, labels = eval_pred.predictions, eval_pred.label_ids
        
        # tokenizer ê°€ì ¸ì˜¤ê¸°
        tok = getattr(self, "processing_class", None) or getattr(self, "tokenizer", None)
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        metrics = {}
        
        # PPL ê³„ì‚°
        if hasattr(eval_pred, 'metrics') and 'eval_loss' in eval_pred.metrics:
            metrics['eval_perplexity'] = calculate_perplexity(eval_pred.metrics['eval_loss'])
        
        # ìƒì„± ë©”íŠ¸ë¦­ ê³„ì‚° (predict_with_generate=True í•„ìš”)
        if isinstance(preds, np.ndarray) and preds.ndim > 1:
            try:
                # ì•ˆì „í•œ ë””ì½”ë”©ì„ ìœ„í•´ safe_batch_decode ì‚¬ìš©
                preds_txt = safe_batch_decode(tok, preds)
                lbl = labels.copy()
                lbl[lbl == -100] = tok.pad_token_id
                refs_txt = safe_batch_decode(tok, lbl)
                
                # ìƒì„± ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì¶”ê°€
                gen_m = generation_metrics(preds_txt, refs_txt)
                metrics.update(gen_m)
                
            except Exception as e:
                logger.warning(f"ìƒì„± ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ê¸°ë³¸ê°’ìœ¼ë¡œ 0 ì„¤ì •
                metrics.update({
                    "bleu1": 0.0, "bleu2": 0.0, "bleu3": 0.0, "bleu4": 0.0,
                    "rouge_l": 0.0, "meteor": 0.0, "cider": 0.0
                })
        
        # strategy metrics ê³„ì‚°
        if len(self.strategy_logits_cache) > 0:
            logits = np.concatenate(self.strategy_logits_cache, axis=0)
            sid_gt = np.concatenate(self.sid_cache, axis=0)
            sid_pred = np.argmax(logits, axis=1)
            acc = accuracy_score(sid_gt, sid_pred)
            f1 = f1_score(sid_gt, sid_pred, average="weighted")
            metrics.update({"strategy_accuracy": acc, "strategy_f1": f1})
            
            # ë‹¤ìŒ í‰ê°€ë¥¼ ìœ„í•´ ìºì‹œ ì •ë¦¬
            self.strategy_logits_cache.clear()
            self.sid_cache.clear()
        
        # BLEU-1ê³¼ strategy_accuracyê°€ ìˆë‹¤ë©´ ë¡œê·¸ ì¶œë ¥
        if "bleu1" in metrics and "strategy_accuracy" in metrics:
            # ì´ ë¶€ë¶„ì„ ìˆ˜ì • - epochê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
            if hasattr(self.state, "epoch") and self.state.epoch is not None:
                epoch_info = f"Epoch {self.state.epoch:.2f}"
            else:
                epoch_info = "Evaluation"
            logger.info(
                f"ğŸ“Š {epoch_info} ë©”íŠ¸ë¦­: BLEU-1={metrics['bleu1']:.4f}, "
                f"Strategy Accuracy={metrics['strategy_accuracy']:.4f}"
            )
        
        # ----------------------- Best Model ê´€ë¦¬ ë¡œì§ ----------------------
        # ë§¤ evaluationë§ˆë‹¤ metricì„ í™•ì¸í•˜ê³  best modelì´ ë°œê²¬ë˜ë©´ ì¦‰ì‹œ ì €ì¥
        current_metric = metrics.get(self.metric_for_best_model, None)
        
        # í•­ìƒ í˜„ì¬ best model ì €ì¥ (ì²« evalì—ì„œëŠ” ë¬´ì¡°ê±´ best)
        if current_metric is None:
            logger.info(f"ì²« ë²ˆì§¸ eval ê²°ê³¼, ëª¨ë¸ ì €ì¥ (metric ì—†ìŒ)")
            self._save_best_model()
        else:
            # current_metricì´ ìˆìœ¼ë©´ ë” ì¢‹ì€ì§€ ë¹„êµ
            is_better = self.greater_is_better and current_metric > self.best_metric
            is_better = is_better or (not self.greater_is_better and current_metric < self.best_metric)
            
            if is_better:
                old_metric = self.best_metric
                self.best_metric = current_metric
                logger.info(f"ìƒˆë¡œìš´ Best ëª¨ë¸ ë°œê²¬! {self.metric_for_best_model}: {old_metric:.4f} â†’ {current_metric:.4f}")
                
                # best model ì¦‰ì‹œ ì €ì¥ (ë””ë ‰í„°ë¦¬ëŠ” ë®ì–´ì“°ê¸°)
                self._save_best_model()
            else:
                logger.info(f"ë” ì¢‹ì€ ëª¨ë¸ ì•„ë‹˜: í˜„ì¬ {self.metric_for_best_model}={current_metric:.4f}, ìµœê³ ={self.best_metric:.4f}")
                
                # best_model ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ í˜„ì¬ ëª¨ë¸ ì €ì¥ (ì²« ì‹¤í–‰ ì‹œ ëŒ€ë¹„)
                if not os.path.exists(os.path.join(self.best_model_dir, "pytorch_model.bin")):
                    logger.warning(f"Best ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ í˜„ì¬ ëª¨ë¸ ì €ì¥")
                    self._save_best_model()
        # ---------------------------------------------------------------------

        return metrics

    # custom caches
    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):  # type: ignore
        """Seq2SeqTrainer â” (loss, generated_tokens, labels) ë°˜í™˜ ì»¤ìŠ¤í…€.

        1. super().prediction_step ë¡œ loss / generated_tokens / labels ë¥¼ êµ¬í•œë‹¤.
        2. ê°™ì€ ì…ë ¥ìœ¼ë¡œ model(**inputs) ì„ í•œ ë²ˆ ë” í˜¸ì¶œí•´ strategy_logits ë¥¼ ì¶”ì¶œí•œë‹¤.
           (generate stepì—ì„œëŠ” strategy_logits ë¥¼ ì–»ì„ ìˆ˜ ì—†ê¸° ë•Œë¬¸)
        """
        if not hasattr(self, "strategy_logits_cache"):
            self.strategy_logits_cache, self.sid_cache = [], []

        # ---------------------------------------------
        # 1) strategy_id ë¶„ë¦¬ (metric ê³„ì‚°ìš©) & ì•ˆì „ ë³µì‚¬
        # ---------------------------------------------
        inputs = inputs.copy()  # ì†ìƒ ë°©ì§€
        sid = inputs.pop("strategy_id")  # Tensor shape (B,)

        # ---------------------------------------------
        # 2) ê¸°ì¡´ Seq2SeqTrainer prediction_step ì‹¤í–‰
        #    loss, generated_tokens, labels íšë“
        # ---------------------------------------------
        loss, generated_tokens, labels = super().prediction_step(
            model, inputs, prediction_loss_only=False, ignore_keys=ignore_keys
        )

        # ---------------------------------------------
        # 3) strategy_logits ì¶”ì¶œ (no_grad ë¡œ forward)
        # ---------------------------------------------
        with torch.no_grad():
            out = model(**inputs)
            if hasattr(out, "strategy_logits"):
                self.strategy_logits_cache.append(out.strategy_logits.cpu().numpy())
            else:
                # fallback: classifier ê²°ê³¼ ì§ì ‘ ê³„ì‚°
                enc_cls = out.encoder_last_hidden_state[:, 0]
                logits = model.classifier(enc_cls).cpu().numpy()
                self.strategy_logits_cache.append(logits)

        self.sid_cache.append(sid.cpu().numpy())

        # ---------------------------------------------
        # 4) ë°˜í™˜ (generated_tokens ëŠ” None ê°€ëŠ¥)
        # ---------------------------------------------
        return loss, generated_tokens, labels

    # ------------------------------------------------------------------
    # safetensors(shared-tensor) RuntimeError íšŒí”¼ìš© ì»¤ìŠ¤í…€ ì €ì¥ í•¨ìˆ˜
    # Trainer.save_model â†’ self._save ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ, ì—¬ê¸°ì„œ safe_serialization=False ë¡œ ì €ì¥
    # ------------------------------------------------------------------
    def _save(self, output_dir: str | None = None, state_dict=None):  # type: ignore
        """
        ìµœì í™”ëœ ì €ì¥ ë¡œì§:
        1. ì²´í¬í¬ì¸íŠ¸(-NNNN)ì—ëŠ” ëª¨ë¸ ì €ì¥í•˜ì§€ ì•ŠìŒ (metricsë§Œ ìœ ì§€)
        2. best_model ë””ë ‰í† ë¦¬ì— í•­ìƒ í˜„ì¬ê¹Œì§€ì˜ best model ìœ ì§€
        3. ë§ˆì§€ë§‰ì—ëŠ” í•™ìŠµ ì¢…ë£Œ ì‹œ best ëª¨ë¸ ë³µì›í•´ì„œ ìµœì¢… ì €ì¥
        """
        import os, logging, torch

        logger = logging.getLogger("joint")
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)

        # ì²´í¬í¬ì¸íŠ¸ íŒë‹¨: checkpoint-NNNN í˜•íƒœì˜ ê²½ë¡œì¸ì§€ í™•ì¸
        is_checkpoint = "checkpoint-" in os.path.basename(output_dir)
        # best model ë””ë ‰í† ë¦¬ íŒë‹¨
        is_best_model_dir = output_dir == self.best_model_dir
        
        if is_checkpoint:
            logger.info(f"[JointTrainer] ì²´í¬í¬ì¸íŠ¸ëŠ” metricsë§Œ ì €ì¥: {output_dir}")
            # ì²´í¬í¬ì¸íŠ¸ì—ëŠ” metricsë§Œ ìœ ì§€í•˜ê³  ëª¨ë¸ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
            # state ì¶”ì ìš© json íŒŒì¼ì€ Trainerê°€ ë³„ë„ë¡œ ì €ì¥
            return

        model_to_save = self.model
        if hasattr(model_to_save, "module"):
            model_to_save = model_to_save.module  # unwrap DDP

        # -----------------------------------------
        # 1) ë³¸ì²´(BART) ì €ì¥
        # -----------------------------------------
        if hasattr(model_to_save, "save_pretrained"):
            # JointBart ëŠ” ìì²´ save_pretrained ê°€ ì—†ìœ¼ë¯€ë¡œ BartForConditionalGenerationì— ëŒ€í•´ í˜¸ì¶œ
            model_to_save.save_pretrained(output_dir, safe_serialization=False, state_dict=state_dict)
        elif hasattr(model_to_save, "model") and hasattr(model_to_save.model, "save_pretrained"):
            model_to_save.model.save_pretrained(output_dir, safe_serialization=False, state_dict=state_dict)
        else:
            # fallback: state_dict ì „ì²´ë¥¼ ì¼ë°˜ torch.save
            torch.save(model_to_save.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))

        # -----------------------------------------
        # 2) ë¶„ë¥˜ í—¤ë“œ ë³„ë„ ì €ì¥ (state_dict)
        # -----------------------------------------
        try:
            torch.save(model_to_save.classifier.state_dict(), os.path.join(output_dir, "classifier.bin"))
        except Exception:
            pass  # classifier ì—†ì„ ìˆ˜ë„ ìˆìŒ

        # -----------------------------------------
        # 3) tokenizerì™€ ê¸°íƒ€ ì •ë³´ ì €ì¥
        # -----------------------------------------
        if hasattr(self, "tokenizer") and self.tokenizer is not None:
            try:
                self.tokenizer.save_pretrained(output_dir)
                logger.info(f"[JointTrainer] tokenizer saved to {output_dir}")
            except Exception as e:
                logger.warning(f"tokenizer save failed: {e}")

            # training args ì €ì¥
            torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
            logger.info(f"[JointTrainer] training args saved to {output_dir}")

        logger.info(f"[JointTrainer] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir} {'(Best Model)' if is_best_model_dir else ''}")

    # ------------------------------------------------------------------
    # Best ëª¨ë¸ ì €ì¥ - compute_metricsì—ì„œ ìƒˆë¡œìš´ best ëª¨ë¸ ë°œê²¬ ì‹œ í˜¸ì¶œë¨
    # ------------------------------------------------------------------
    def _save_best_model(self):
        """í˜„ì¬ ëª¨ë¸ì„ best_model ë””ë ‰í† ë¦¬ì— ì €ì¥"""
        import logging
        logger = logging.getLogger("joint")
        
        logger.info(f"Best ëª¨ë¸ ì €ì¥ ì‹œì‘: {self.best_model_dir}")
        # _save ë©”ì†Œë“œë¥¼ í™œìš©í•´ best_model ë””ë ‰í† ë¦¬ì— ì €ì¥
        # checkpoint ê²½ë¡œê°€ ì•„ë‹ˆë¯€ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì €ì¥ë¨
        self._save(output_dir=self.best_model_dir)
        logger.info(f"Best ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.best_model_dir}")

    # ------------------------------------------------------------------
    # ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥í•˜ì§€ ì•Šë„ë¡ ì˜¤ë²„ë¼ì´ë“œ
    # ì´ë¡œì¨ ê° ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì•½ 1GBì˜ ê³µê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆìŒ
    # ------------------------------------------------------------------
    def _save_optimizer_and_scheduler(self, output_dir: str):
        """
        ì²´í¬í¬ì¸íŠ¸ì—ëŠ” optimizer.ptì™€ scheduler.ptë¥¼ ì €ì¥í•˜ì§€ ì•Šê³ 
        ìµœì¢… ëª¨ë¸ì—ë„ ê¸°ë³¸ì ìœ¼ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì €ì¥ ì‹¤íŒ¨ ì‹œ ìë™ ë¬´ì‹œ)
        """
        import os, logging
        logger = logging.getLogger("joint")

        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì¸ì§€ í™•ì¸
        is_checkpoint = "checkpoint-" in os.path.basename(output_dir)
        
        if is_checkpoint:
            # ì²´í¬í¬ì¸íŠ¸ì—ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ - ê³µê°„ ì ˆì•½
            logger.info(f"[JointTrainer] ì²´í¬í¬ì¸íŠ¸ì— optimizer/scheduler ì €ì¥ ì•ˆí•¨ ({output_dir})")
            return
        
        # ìµœì¢… ëª¨ë¸ì—ëŠ” ì €ì¥ ì‹œë„ (ì›ë˜ ë¡œì§)
        try:
            super()._save_optimizer_and_scheduler(output_dir)
            logger.info(f"[JointTrainer] optimizer/scheduler saved to {output_dir}")
        except RuntimeError as e:
            logger.warning(f"optimizer/scheduler save failed â†’ ê±´ë„ˆëœ€: {e}")

# ============================== main ================================================

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--epochs", type=int, default=3)
    ap.add_argument("--batch_size", type=int, default=8)
    ap.add_argument("--lambda_cls", type=float, default=0.5)
    ap.add_argument("--lr", type=float, default=5e-5)
    ap.add_argument("--output_dir", type=str, default="runs/joint")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--tiny_frac", type=float, default=None, help="ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ì‚¬ìš© (e.g., 0.01 = 1%)")
    ap.add_argument("--patience", type=int, default=3, help="early stopping patience (number of epochs without improvement)")
    ap.add_argument("--clean_checkpoints", action="store_true", help="í•™ìŠµ ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ í´ë” ì •ë¦¬")
    ap.add_argument("--eval_init", action="store_true", help="í•™ìŠµ ì „ ì´ˆê¸° ëª¨ë¸(epoch 0)ì—ì„œ í‰ê°€ ìˆ˜í–‰")
    args = ap.parse_args()

    # logging
    os.makedirs(args.output_dir, exist_ok=True)
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s",
                        handlers=[logging.FileHandler(Path(args.output_dir)/"train.log"), logging.StreamHandler()])

    set_seed(args.seed)
    tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
    tokenizer.add_special_tokens({"additional_special_tokens": list(SPECIAL_TOKENS.values()) + STRAT_TOKENS})

    train_ds = JointESConvDataset("train", tokenizer)
    val_ds   = JointESConvDataset("validation", tokenizer)
    test_ds  = JointESConvDataset("test", tokenizer)

    # -------------------------------------------------------------
    # tiny training option: ë§¤ìš° ì‘ì€ ë¶€ë¶„ì§‘í•©ìœ¼ë¡œ epoch ì†ë„ í…ŒìŠ¤íŠ¸
    # -------------------------------------------------------------
    if args.tiny_frac is not None and 0 < args.tiny_frac < 1:
        import random
        def _subset(ds, frac):
            n = max(1, int(len(ds) * frac))
            idx = random.sample(range(len(ds)), n)
            ds.examples = [ds.examples[i] for i in idx]
            return ds
        train_ds = _subset(train_ds, args.tiny_frac)
        val_ds   = _subset(val_ds, args.tiny_frac)
        test_ds  = _subset(test_ds, args.tiny_frac)  # í…ŒìŠ¤íŠ¸ì…‹ë„ ë™ì¼í•˜ê²Œ ì ìš©
        logging.info(f"[tiny] train={len(train_ds)}  val={len(val_ds)}  test={len(test_ds)} examples (fraction={args.tiny_frac})")

    model = JointBart(tokenizer, lambda_cls=args.lambda_cls)

    data_collator = DataCollatorForSeq2Seq(tokenizer, label_pad_token_id=-100, pad_to_multiple_of=8)

    train_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        learning_rate=args.lr,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=1,    # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ 1ê°œë§Œ ìœ ì§€
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        predict_with_generate=True,
        generation_max_length=64,
        report_to="none",
    )

    # checkpointì— ëª¨ë¸ì´ ì €ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê° í‰ê°€ë§ˆë‹¤ best_model ë””ë ‰í† ë¦¬ì— ì €ì¥í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±
    class SaveBestModelCallback(TrainerCallback):
        def on_evaluate(self, args, state, control, metrics=None, **kwargs):
            if metrics is None:
                return
            
            metric_to_check = args.metric_for_best_model
            if not metric_to_check:
                return
                
            metric_value = metrics.get(metric_to_check)
            if metric_value is None:
                return
                
            # trainer ê°ì²´ ê°€ì ¸ì˜¤ê¸°
            trainer = kwargs.get("trainer", None)
            if trainer is None:
                return
                
            # best_model_dirì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ìƒì„±
            if not hasattr(trainer, "best_model_dir"):
                trainer.best_model_dir = os.path.join(args.output_dir, "best_model")
                
            # best_metric ì´ˆê¸°í™”
            if not hasattr(trainer, "best_metric"):
                trainer.best_metric = float("inf") if not args.greater_is_better else float("-inf")
                
            operator = np.greater if args.greater_is_better else np.less
            
            if operator(trainer.best_metric, metric_value):
                # ë©”íŠ¸ë¦­ì´ ë” ë‚˜ë¹ ì¡Œìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
                return
                
            # ì €ì¥ ë¡œì§
            logger.info(f"ìƒˆë¡œìš´ Best ëª¨ë¸ ë°œê²¬ ({metric_to_check}={metric_value}), ì €ì¥í•©ë‹ˆë‹¤.")
            trainer.best_metric = metric_value
            trainer.save_model(trainer.best_model_dir)
            
    # ì½œë°± ëª©ë¡ ìƒì„±
    callbacks = [
        EarlyStoppingCallback(early_stopping_patience=args.patience),
        SaveBestModelCallback(),
    ]
    
    trainer = JointTrainer(
        model=model,
        args=train_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        data_collator=data_collator,
        callbacks=callbacks,
    )

    # --------------------------- ì´ˆê¸° ëª¨ë¸ (epoch 0) í‰ê°€ ---------------------------
    if args.eval_init:
        from sklearn.metrics import accuracy_score, f1_score, classification_report 
        
        logging.info("ì´ˆê¸° ëª¨ë¸(epoch 0) í‰ê°€ ì‹œì‘...")
        
        # -------------------------------------------------------------
        # (1) ê¸°ë³¸ loss ë“±               : trainer.evaluate
        # (2) ìƒì„± & ì „ëµ ë©”íŠ¸ë¦­ ê³„ì‚°     : ì§ì ‘ predict í›„ ê³„ì‚°
        # -------------------------------------------------------------

        # 1) loss ë“± ê¸°ë³¸ ë©”íŠ¸ë¦­
        init_val_metrics = trainer.evaluate(val_ds, metric_key_prefix="init_val")
        logging.info(f"ì´ˆê¸° ëª¨ë¸ validation í‰ê°€ ê²°ê³¼: loss={init_val_metrics['init_val_loss']:.4f}")
        
        # validation PPL ê³„ì‚°
        if "init_val_loss" in init_val_metrics:
            init_val_metrics["init_val_perplexity"] = calculate_perplexity(init_val_metrics["init_val_loss"])
            logging.info(f"ì´ˆê¸° ëª¨ë¸ Validation Perplexity: {init_val_metrics['init_val_perplexity']:.4f}")
        
        # í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë³¸ ë©”íŠ¸ë¦­
        init_test_metrics = trainer.evaluate(test_ds, metric_key_prefix="init_test")
        
        # í…ŒìŠ¤íŠ¸ PPL ê³„ì‚°
        if "init_test_loss" in init_test_metrics:
            init_test_metrics["init_test_perplexity"] = calculate_perplexity(init_test_metrics["init_test_loss"])
            logging.info(f"ì´ˆê¸° ëª¨ë¸ Test Perplexity: {init_test_metrics['init_test_perplexity']:.4f}")
        
        # 2) ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        logging.info("ì´ˆê¸° ëª¨ë¸ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± & ì „ëµ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        preds_init = trainer.predict(test_ds, metric_key_prefix="init_test")
        
        # generation metrics
        gen_texts_init = safe_batch_decode(tokenizer, preds_init.predictions)
        # label_ids â†’ refs
        lbl_ids = preds_init.label_ids.copy()
        lbl_ids[lbl_ids == -100] = tokenizer.pad_token_id
        refs_init = safe_batch_decode(tokenizer, lbl_ids)
        gen_metrics_init = generation_metrics(gen_texts_init, refs_init)
        
        # ìƒì„± ë©”íŠ¸ë¦­ ê²°ê³¼ ì¶œë ¥
        logging.info(f"ì´ˆê¸° ëª¨ë¸ ìƒì„± ì„±ëŠ¥: BLEU={gen_metrics_init.get('bleu', 0):.4f}, ROUGE-L={gen_metrics_init.get('rouge_l', 0):.4f}")
        
        # strategy metrics (accuracy / weighted f1)
        logging.info("ì´ˆê¸° ëª¨ë¸ ì „ëµ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        strat_loader = torch.utils.data.DataLoader(
            test_ds,
            batch_size=args.batch_size,
            collate_fn=data_collator,
            shuffle=False,
        )
        all_sid_pred_init, all_sid_gt_init = [], []
        device_eval = next(model.parameters()).device
        with torch.no_grad():
            for batch in strat_loader:
                sid_gt = batch.pop("strategy_id")
                input_ids = batch["input_ids"].to(device_eval)
                attention_mask = batch["attention_mask"].to(device_eval)
                outs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outs.strategy_logits
                sid_pred = torch.argmax(logits, dim=-1).cpu()
                all_sid_pred_init.append(sid_pred)
                all_sid_gt_init.append(sid_gt)
        
        all_sid_pred_init = torch.cat(all_sid_pred_init).numpy()
        all_sid_gt_init = torch.cat(all_sid_gt_init).numpy()
        strat_acc_init = accuracy_score(all_sid_gt_init, all_sid_pred_init)
        strat_f1_init = f1_score(all_sid_gt_init, all_sid_pred_init, average="weighted")
        
        # ì „ëµ ë¶„ë¥˜ ê²°ê³¼ ì¶œë ¥
        logging.info(f"ì´ˆê¸° ëª¨ë¸ ì „ëµ ë¶„ë¥˜ ì„±ëŠ¥: Accuracy={strat_acc_init:.4f}, F1={strat_f1_init:.4f}")
        
        # ì „ëµë³„ ìƒì„¸ ì„±ëŠ¥ ë³´ê³ ì„œ
        from utils.strategy import STRATEGIES
        init_strat_report = classification_report(
            all_sid_gt_init, all_sid_pred_init,
            labels=list(range(len(STRATEGIES))),
            target_names=STRATEGIES,
            digits=2,
            zero_division=0
        )
        logging.info("\n=== ì´ˆê¸° ëª¨ë¸ ì „ëµ ë¶„ë¥˜ ìƒì„¸ ë³´ê³ ì„œ ===\n" + init_strat_report)
        
        # ë©”íŠ¸ë¦­ í†µí•©
        init_test_metrics.update({f"init_test_{k}": float(v) for k, v in gen_metrics_init.items()})
        init_test_metrics.update({
            "init_test_strategy_accuracy": float(strat_acc_init),
            "init_test_strategy_f1": float(strat_f1_init),
        })
        
        # ì´ˆê¸° í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì €ì¥
        Path(args.output_dir).mkdir(exist_ok=True, parents=True)
        
        # validation ë©”íŠ¸ë¦­ ì €ì¥
        init_val_path = Path(args.output_dir) / "init_val_metrics.json"
        with init_val_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) for k, v in init_val_metrics.items()}, f, indent=2)
        
        # test ë©”íŠ¸ë¦­ ì €ì¥
        init_test_path = Path(args.output_dir) / "init_test_metrics.json"
        with init_test_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) for k, v in init_test_metrics.items()}, f, indent=2)
        
        # ìƒ˜í”Œ ì €ì¥
        sample_n = min(10, len(gen_texts_init))
        with open(Path(args.output_dir) / "init_samples.txt", "w", encoding="utf-8") as f:
            for ref, gen in zip(refs_init[:sample_n], gen_texts_init[:sample_n]):
                f.write(f"REF: {ref}\nGEN: {gen}\n---\n")
        
        logging.info(f"ì´ˆê¸° ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.output_dir}")

    # --------------------------- train + save best ---------------------------------
    trainer.train()

    if trainer.is_world_process_zero():
        # í•™ìŠµ ì¢…ë£Œ í›„ í˜„ì¬ ëª¨ë¸ì„ best_model ë””ë ‰í† ë¦¬ì— ê°•ì œ ì €ì¥
        best_model_dir = os.path.join(args.output_dir, "best_model")
        logging.info(f"í•™ìŠµ ì™„ë£Œ: í˜„ì¬ ëª¨ë¸ì„ {best_model_dir}ì— ê°•ì œ ì €ì¥í•©ë‹ˆë‹¤.")
        trainer.save_model(best_model_dir)
        
        # validation metrics dump
        metrics = trainer.evaluate()
        
        # ì†ì‹¤ê°’ì—ì„œ PPL ê³„ì‚° ì¶”ê°€
        if "eval_loss" in metrics:
            metrics["eval_perplexity"] = calculate_perplexity(metrics["eval_loss"])
            logging.info(f"Validation Perplexity: {metrics['eval_perplexity']:.4f}")
            
        metrics_path = Path(args.output_dir) / "val_metrics.json"
        with metrics_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) for k, v in metrics.items()}, f, indent=2)

        # ì „ëµ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥ - validation ë°ì´í„°ì…‹
        logging.info("[main] computing strategy classification report for validation set...")
        model.eval()
        val_loader = torch.utils.data.DataLoader(
            val_ds,  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” validation ë°ì´í„°ì…‹ ì‚¬ìš©
            batch_size=args.batch_size,
            collate_fn=data_collator,
            shuffle=False,
        )
        val_sid_pred, val_sid_gt = [], []
        device_eval = next(model.parameters()).device
        with torch.no_grad():
            for batch in val_loader:
                sid_gt = batch.pop("strategy_id")
                input_ids = batch["input_ids"].to(device_eval)
                attention_mask = batch["attention_mask"].to(device_eval)
                outs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outs.strategy_logits
                sid_pred = torch.argmax(logits, dim=-1).cpu()
                val_sid_pred.append(sid_pred)
                val_sid_gt.append(sid_gt)

        val_sid_pred = torch.cat(val_sid_pred).numpy()
        val_sid_gt = torch.cat(val_sid_gt).numpy()

        # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥
        val_report = classification_report(
            val_sid_gt, val_sid_pred,
            labels=list(range(len(STRATEGIES))),
            target_names=STRATEGIES,
            digits=2,
            zero_division=0
        )
        logging.info("\n=== Validation Strategy Classification Report ===\n" + val_report)

        # -------------------- TEST DATASET EVALUATION --------------------
        logging.info("[main] evaluating on test split â€¦")
        test_ds = JointESConvDataset("test", tokenizer)

        # tiny training ëª¨ë“œë¼ë©´ test ì„¸íŠ¸ë„ ë™ì¼ ë¹„ìœ¨ ì„œë¸Œìƒ˜í”Œ
        if args.tiny_frac is not None and 0 < args.tiny_frac < 1:
            import random
            n_test = max(1, int(len(test_ds) * args.tiny_frac))
            idx = random.sample(range(len(test_ds)), n_test)
            test_ds.examples = [test_ds.examples[i] for i in idx]
            logging.info(f"[tiny] test={len(test_ds)} examples (fraction={args.tiny_frac})")

        # -------------------------------------------------------------
        # (1) ê¸°ë³¸ loss ë“±               : trainer.evaluate
        # (2) ìƒì„± & ì „ëµ ë©”íŠ¸ë¦­ ê³„ì‚°     : ì§ì ‘ predict í›„ ê³„ì‚°
        # -------------------------------------------------------------

        # 1) loss ë“± ê¸°ë³¸ ë©”íŠ¸ë¦­
        test_metrics = trainer.evaluate(test_ds, metric_key_prefix="test")

        # ì†ì‹¤ê°’ì—ì„œ PPL ê³„ì‚° ì¶”ê°€
        if "test_loss" in test_metrics:
            test_metrics["test_perplexity"] = calculate_perplexity(test_metrics["test_loss"])
            logging.info(f"Test Perplexity: {test_metrics['test_perplexity']:.4f}")

        # 2) ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        logging.info("[main] computing generation & strategy metrics on test â€¦")
        preds_full = trainer.predict(test_ds)

        # generation metrics
        gen_texts_full = safe_batch_decode(tokenizer, preds_full.predictions)
        # label_ids â†’ refs
        lbl_ids = preds_full.label_ids.copy()
        lbl_ids[lbl_ids == -100] = tokenizer.pad_token_id
        refs_full = safe_batch_decode(tokenizer, lbl_ids)
        gen_metrics = generation_metrics(gen_texts_full, refs_full)

        # strategy metrics (accuracy / weighted f1)
        from sklearn.metrics import accuracy_score, f1_score
        logging.info("[main] computing strategy metrics â€¦")
        model.eval()
        strat_loader = torch.utils.data.DataLoader(
            test_ds,
            batch_size=args.batch_size,
            collate_fn=data_collator,
            shuffle=False,
        )
        all_sid_pred, all_sid_gt = [], []
        device_eval = next(model.parameters()).device
        with torch.no_grad():
            for batch in strat_loader:
                sid_gt = batch.pop("strategy_id")
                input_ids = batch["input_ids"].to(device_eval)
                attention_mask = batch["attention_mask"].to(device_eval)
                outs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outs.strategy_logits
                sid_pred = torch.argmax(logits, dim=-1).cpu()
                all_sid_pred.append(sid_pred)
                all_sid_gt.append(sid_gt)

        all_sid_pred = torch.cat(all_sid_pred).numpy()
        all_sid_gt = torch.cat(all_sid_gt).numpy()
        strat_acc = accuracy_score(all_sid_gt, all_sid_pred)
        strat_f1 = f1_score(all_sid_gt, all_sid_pred, average="weighted")

        # ë©”íŠ¸ë¦­ í†µí•©
        test_metrics.update({f"test_{k}": float(v) for k, v in gen_metrics.items()})
        test_metrics.update({
            "test_strategy_accuracy": float(strat_acc),
            "test_strategy_f1": float(strat_f1),
        })

        # ì €ì¥
        test_path = Path(args.output_dir) / "test_metrics.json"
        with test_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) for k, v in test_metrics.items()}, f, indent=2)

        # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥
        test_report = classification_report(
            all_sid_gt, all_sid_pred,
            labels=list(range(len(STRATEGIES))),
            target_names=STRATEGIES,
            digits=2,
            zero_division=0
        )
        logging.info("\n=== Test Strategy Classification Report ===\n" + test_report)

        # ìƒ˜í”Œ 10ê°œ ì €ì¥ (validation ìƒ˜í”Œ íŒŒì¼ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ)
        sample_n_test = min(10, len(gen_texts_full))
        sample_path = Path(args.output_dir) / "samples.txt"
        with sample_path.open("w", encoding="utf-8") as f:
            for ref, gen in zip(refs_full[:sample_n_test], gen_texts_full[:sample_n_test]):
                f.write(f"REF: {ref}\nGEN: {gen}\n---\n")
        logging.info(f"saved best model + metrics and samples to {args.output_dir}")

    # ----------------------------- cleanup ----------------------------
    if torch.distributed.is_available() and torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()
    # ------------------------------------------------------------------------------

# ------------------------ helper: safe decode ------------------------
def safe_batch_decode(tokenizer: BartTokenizer, predictions):
    """í† í¬ë‚˜ì´ì € ì˜¤ë¥˜(NoneType) ë°©ì§€ë¥¼ ìœ„í•´ id ë²”ìœ„ë¥¼ ê²€ì‚¬í•˜ë©° ë””ì½”ë“œ."""
    texts: list[str] = []
    vocab_size = len(tokenizer)
    
    for seq in predictions:
        try:
            # seq ê°€ numpy array / list / torch í…ì„œ ëª¨ë‘ ì§€ì›
            if isinstance(seq, torch.Tensor):
                seq = seq.tolist()
            elif isinstance(seq, np.ndarray):
                seq = seq.tolist()
                
            # id ë²”ìœ„ ë°– ê°’, None, ë˜ëŠ” ë¹„ì •ìˆ˜ê°’ì„ unk í† í°ìœ¼ë¡œ ëŒ€ì²´
            clean_ids = []
            for t in seq:
                if isinstance(t, (int, np.integer)) and 0 <= int(t) < vocab_size:
                    clean_ids.append(int(t))
                else:
                    clean_ids.append(tokenizer.unk_token_id)
                    
            text = tokenizer.decode(clean_ids, skip_special_tokens=True)
            texts.append(text)
            
        except Exception as e:
            # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ì¶”ê°€
            texts.append("")
            logging.warning(f"í† í° ë””ì½”ë”© ì‹¤íŒ¨: {e}")
            
    return texts

if __name__ == "__main__":
    main() 