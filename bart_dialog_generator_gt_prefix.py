# -*- coding: utf-8 -*-
"""
bart_dialog_generator_gt_prefix.py
============================
BART ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ë§¥ë½ê³¼ ì „ëµ(strategy)ì„ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” íŒŒì´í”„ë¼ì¸.

ì‚¬ìš©ë²• ì˜ˆì‹œ:
----------
# ê¸°ë³¸ í›ˆë ¨
CUDA_VISIBLE_DEVICES=1 python bart_dialog_generator_gt_prefix.py --batch_size 16 --output_dir outputs/dialog_generation_gt_prefix --no_save_optimizer

# ì‘ì€ ë¹„ìœ¨ì˜ ë°ì´í„°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
CUDA_VISIBLE_DEVICES=0 python bart_dialog_generator_gt_prefix.py --tiny_frac 0.05 --epochs 1 --eval_steps 10 --output_dir outputs/dialog_tiny_gt_prefix --no_save_optimizer

# facebook/bart-base ì›ë³¸ ëª¨ë¸ í‰ê°€
CUDA_VISIBLE_DEVICES=2 python bart_dialog_generator_gt_prefix.py --eval_only --output_dir outputs/dialog_eval_gt_prefix

# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ì‚¬ìš©í•œ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ í•™ìŠµ (ì‹¤íš¨ ë°°ì¹˜ í¬ê¸° 32)
CUDA_VISIBLE_DEVICES=3 python bart_dialog_generator_gt_prefix.py --gradient_accumulation_steps 2 --output_dir outputs/dialog_batch_32_gt_prefix
"""

from __future__ import annotations

import argparse, json, logging, random
from pathlib import Path
from typing import Dict, List, Union, Any, Optional

import numpy as np
import torch
import nltk
from datasets import load_dataset
from transformers import (
    BartTokenizer,
    BartForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback,
    TrainerCallback,
    GenerationConfig,
    set_seed,
    BartConfig,
    PreTrainedTokenizerBase,
)
from utils.metrics import generation_metrics

# ===================== ì „ì—­ ì •ì˜ =====================
SPECIAL_TOKENS = {         
    "usr": "[USR]",
    "sys": "[SYS]",
    "Question": "[STRAT_Question]",
    "Restatement or Paraphrasing": "[STRAT_Paraphrasing]",
    "Reflection of feelings": "[STRAT_Reflection]",
    "Self-disclosure": "[STRAT_SelfDisclosure]",
    "Affirmation and Reassurance": "[STRAT_Reassurance]",
    "Providing Suggestions": "[STRAT_Suggestion]",
    "Information": "[STRAT_Information]",
    "Others": "[STRAT_Others]",
}

logger = logging.getLogger("dialog_gen")

# ===================== ë°ì´í„°ì…‹ =====================
class DialogGenDataset(torch.utils.data.Dataset):
    """
    ëŒ€í™” ìƒì„±ì„ ìœ„í•œ ë°ì´í„°ì…‹.
    
    ëŒ€í™” ë§¥ë½(context)ì„ ì…ë ¥ìœ¼ë¡œ, ì‹œìŠ¤í…œ ì‘ë‹µì„ ì¶œë ¥ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°ì…‹
    """

    def __init__(
        self,
        split: str,
        tokenizer: BartTokenizer,
        max_src: int = 1024,
        max_tgt: int = 256,
        tiny_frac: float | None = None,
        cache_dir: str = "cache_dialog_gen_gt_prefix",
        dataset_name: str = "thu-coai/esconv",
        use_cache: bool = True,
        bos_is_strategy: bool = False,
    ):
        self.tok = tokenizer
        self.max_src, self.max_tgt = max_src, max_tgt
        self.dataset_name = dataset_name
        self.bos_is_strategy = bos_is_strategy

        # ë°ì´í„°ì…‹ ë¡œë“œ
        raw = load_dataset(dataset_name, split=split)

        # ì‘ì€ ë¹„ìœ¨ë§Œ ì‚¬ìš© (ë””ë²„ê¹…ìš©)
        if tiny_frac:
            raw = raw.shuffle(seed=42).select(range(int(len(raw) * tiny_frac)))

        cache_f = (
            Path(cache_dir)
            / f"{split}_{max_src}_{max_tgt}_{tiny_frac}_{dataset_name.replace('/', '_')}.pt"
        )
        # ìºì‹œ ì‚¬ìš©ì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ìºì‹œ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
        if use_cache and cache_f.exists():
            # ë³´ì•ˆ ê²½ê³  í•´ê²°: torch.save/load ì‹œ ê°ì²´ ì§ë ¬í™” ëŒ€ì‹  pickle ì‚¬ìš©
            import pickle
            with open(cache_f, 'rb') as f:
                self.examples = pickle.load(f)
            return

        self.examples: List[Dict] = []
        for ex in raw:
            dialog = json.loads(ex["text"])["dialog"]
            for turn_idx, turn in enumerate(dialog):
                if turn["speaker"] != "sys":
                    continue
                
                # 1) contextë¥¼ </s>ë¡œ êµ¬ë¶„
                ctx_parts = []
                for prev in dialog[:turn_idx]:
                    spk_tok = SPECIAL_TOKENS["usr"] if prev["speaker"] == "usr" else SPECIAL_TOKENS["sys"]
                    if prev["speaker"] == "usr":
                        ctx_parts.append(f"{spk_tok}{prev['text']}")
                    else:
                        strategy = prev["strategy"]
                        ctx_parts.append(f"{spk_tok}{SPECIAL_TOKENS[strategy]}{prev['text']}")

                # íŠ¹ìˆ˜ í† í°ì„ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ í† í¬ë‚˜ì´ì €ì˜ bos/eos í† í°ìœ¼ë¡œ ì‚¬ìš©
                context = tokenizer.bos_token + (tokenizer.eos_token.join(ctx_parts) if ctx_parts else "") + tokenizer.eos_token

                if not context.strip():
                    continue

                # ---------- decoder output (response) ----------
                # 1) prefix í† í° ìˆœì„œ ê²°ì • (ë³€ê²½: ë§¨ ì•ì— EOS ì¶”ê°€í•˜ì—¬ ê¸¸ì´ 3)
                eos_id = tokenizer.eos_token_id
                sys_tok_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS["sys"])
                strat_tok_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[turn["strategy"]])

                if self.bos_is_strategy:
                    prefix_ids = [eos_id, strat_tok_id, sys_tok_id]
                else:
                    prefix_ids = [eos_id, sys_tok_id, strat_tok_id]

                # 1) ì‘ë‹µ ë¬¸ì¥ í† í°í™” (EOS í¬í•¨)
                sent_ids = self.tok(
                    turn["text"],
                    add_special_tokens=False,
                    max_length=self.max_tgt - 4,  # prefix 3 + EOS 1 ë³´ì¥
                    truncation=True,
                ).input_ids

                tokens = sent_ids + [eos_id]  # w1..wn + EOS (ê¸¸ì´ n+1)

                # 2) decoder_input / labels êµ¬ì„± (ê¸¸ì´ ì°¨ 1)
                #   decoder_input :  </s> [SYS] [STRAT] w1 .. wN-1
                #   labels        :          [SYS] [STRAT] w1 .. wN   (ì²« </s> ì˜ˆì¸¡ì€ [SYS])
                decoder_input = prefix_ids + tokens[:-1]  # ë§ˆì§€ë§‰ EOS ì œê±°
                labels        = prefix_ids[1:] + tokens   # prefixì—ë„ loss ë¶€ì—¬ (</s> ì œì™¸)

                # 3) ê¸¸ì´ ë§ì¶¤ (max_tgt)
                # truncate
                decoder_input = decoder_input[: self.max_tgt]
                labels        = labels[: self.max_tgt]

                # pad (decoder_inputì€ labelsë³´ë‹¤ ìµœëŒ€ 1 ì§§ìŒ) -> ë™ì¼ ê¸¸ì´ í›„ max_tgtê¹Œì§€ pad
                while len(decoder_input) < len(labels):
                    decoder_input.append(tokenizer.pad_token_id)

                pad_len = self.max_tgt - len(labels)
                decoder_input += [tokenizer.pad_token_id] * pad_len
                labels        += [-100] * pad_len

                decoder_attn = [1 if tid != tokenizer.pad_token_id else 0 for tid in decoder_input]

                # ----------- ì¸ì½”ë” í† í°í™” -----------
                enc = self.tok(
                    context,
                    max_length=self.max_src,
                    truncation=True,
                    padding="max_length",
                    add_special_tokens=False,
                )

                self.examples.append({
                    "input_ids": enc.input_ids,
                    "attention_mask": enc.attention_mask,
                    "decoder_input_ids": decoder_input,
                    "decoder_attention_mask": decoder_attn,
                    "labels": labels,
                    "context": context,
                    "response": turn["text"],
                })
        # ìºì‹œ ì‚¬ìš©ì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ìºì‹œ íŒŒì¼ ì €ì¥
        if use_cache:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            cache_f.parent.mkdir(exist_ok=True, parents=True)
            # ë³´ì•ˆ ê²½ê³  í•´ê²°: torch.save/load ì‹œ ê°ì²´ ì§ë ¬í™” ëŒ€ì‹  pickle ì‚¬ìš©
            import pickle
            with open(cache_f, 'wb') as f:
                pickle.dump(self.examples, f)

    # -------------- torch Dataset interface --------------
    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        # ì´ë¯¸ í† í°í™”ëœ ì˜ˆì œë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return self.examples[idx]


# ===================== ìœ í‹¸ í•¨ìˆ˜ =====================
def safe_decode(ids, tokenizer, skip_special_tokens=False, **kwargs):
    """ì•ˆì „í•˜ê²Œ ë””ì½”ë”©í•˜ë˜, pad í† í°ë§Œ ì œì™¸í•˜ê³  ë‹¤ë¥¸ special tokenì€ ìœ ì§€"""
    # ë¦¬ìŠ¤íŠ¸ì™€ í…ì„œë¥¼ ëª¨ë‘ í—ˆìš©
    if isinstance(ids, torch.Tensor):
        ids = ids.tolist()

    if not isinstance(ids, list):
        ids = [ids]

    # 1) pad ì´í›„ ìë¥´ê¸°
    if tokenizer.pad_token_id in ids:
        first_pad = ids.index(tokenizer.pad_token_id)
        ids = ids[:first_pad]

    # 2) ìŒìˆ˜/None/ë²”ìœ„ ì´ˆê³¼ ID ì œê±°
    filtered = [int(t) for t in ids if isinstance(t, int) and 0 <= t < len(tokenizer)]

    if not filtered:
        return ""

    try:
        return tokenizer.decode(filtered, skip_special_tokens=skip_special_tokens, **kwargs)
    except Exception as e:
        logger.debug(f"safe_decode ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
        return ""


# ===================== ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„° =====================
class CustomDataCollatorWithDecoderPrefix(DataCollatorForSeq2Seq):
    """
    ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„°: decoder_input_idsì™€ labels ì²˜ë¦¬ë¥¼ ìœ„í•œ íŠ¹ë³„ ë¡œì§
    - decoder_input_ids: </s> [STRAT] [SYS] (ë˜ëŠ” </s> [SYS] [STRAT]) 3-í† í° prefix ìœ ì§€
    - labels: prefix ë¶€ë¶„ -100 ë§ˆìŠ¤í‚¹ ë³´ì¥
    """
    def __init__(
        self,
        tokenizer: PreTrainedTokenizerBase,
        model=None,
        label_pad_token_id: int = -100,
        decoder_prefix_len: int = 0,
        **kwargs
    ):
        super().__init__(tokenizer, model, label_pad_token_id=label_pad_token_id, **kwargs)
        self.decoder_prefix_len = decoder_prefix_len

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # ì›ë³¸ íŒ¨ë”© ì²˜ë¦¬
        batch = super().__call__(features)

        # ë””ë²„ê·¸ ë¡œê¹… (ì²« í˜¸ì¶œì—ì„œë§Œ)
        if not hasattr(self, '_logged_once'):
            logger.info("\n===== ì»¤ìŠ¤í…€ ì½œë ˆì´í„° ë””ë²„ê¹… =====")
            logger.info(f"ë°°ì¹˜ í‚¤: {list(batch.keys())}")
            if 'decoder_input_ids' in batch:
                logger.info(f"decoder_input_ids ìƒ˜í”Œ: {batch['decoder_input_ids'][0][:10].tolist()}")
            if 'labels' in batch:
                logger.info(f"labels ìƒ˜í”Œ: {batch['labels'][0][:10].tolist()}")
            self._logged_once = True

        # labelsì—ì„œ prefix ë¶€ë¶„ì— -100 ë§ˆìŠ¤í‚¹ ë³´ì¥
        if "labels" in batch and "decoder_input_ids" in batch:
            labels = batch["labels"]
            # prefix_len ê¸¸ì´ë§Œí¼ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
            labels[:, :self.decoder_prefix_len] = self.label_pad_token_id
            batch["labels"] = labels

        return batch


# ===================== ë©”ì¸ =====================
def main():
    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_dir", type=str, default="outputs/dialog_gen")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1,
                        help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜ (ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = batch_size * gradient_accumulation_steps)")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--tiny_frac", type=float, default=None, help="0~1: ë””ë²„ê·¸ìš© ìƒ˜í”Œ ë¹„ìœ¨")
    parser.add_argument("--patience", type=int, default=5,
                        help="early stopping patience (eval_loss ê¸°ì¤€)")
    parser.add_argument("--eval_steps", type=int, default=500,
                        help="evaluation interval (steps)")
    parser.add_argument("--show_samples", type=int, default=10,
                        help="ìƒ˜í”Œ nê°œ(context/target) ì¶œë ¥ í›„ ì¢…ë£Œ")
    parser.add_argument("--eval_init", action="store_true", 
                        help="í•™ìŠµ ì „ ì´ˆê¸° ëª¨ë¸(epoch 0)ì—ì„œ í‰ê°€ ìˆ˜í–‰")
    parser.add_argument("--eval_only", action="store_true",
                        help="í•™ìŠµ ì—†ì´ í‰ê°€ë§Œ ìˆ˜í–‰")
    parser.add_argument("--dataset", type=str, default="thu-coai/esconv",
                        help="ì‚¬ìš©í•  ë°ì´í„°ì…‹ (ê¸°ë³¸ê°’: ESConv)")
    parser.add_argument("--learning_rate", type=float, default=3e-5,
                        help="í•™ìŠµë¥  (ê¸°ë³¸ê°’: 3e-5, í° ë°ì´í„°ì…‹ì€ 5e-5ë„ ê°€ëŠ¥)")
    parser.add_argument("--max_src_length", type=int, default=1024,
                        help="ìµœëŒ€ ì†ŒìŠ¤ ê¸¸ì´")
    parser.add_argument("--max_tgt_length", type=int, default=256,
                        help="ìµœëŒ€ íƒ€ê²Ÿ ê¸¸ì´")
    parser.add_argument("--warmup_ratio", type=float, default=0.05,
                        help="warmup ë¹„ìœ¨ (ì „ì²´ ìŠ¤í…ì˜ %, ì´ˆê¸° ë¶ˆì•ˆì •í•œ loss spike ë°©ì§€)")
    parser.add_argument("--num_samples", type=int, default=5, 
                        help="í•™ìŠµ/í‰ê°€ ì¤‘ ì¶œë ¥í•  ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--no_cache", action="store_true",
                        help="ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•­ìƒ ë°ì´í„°ë¥¼ ìƒˆë¡œ ì²˜ë¦¬")
    parser.add_argument("--no_save_optimizer", action="store_true",
                        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œ optimizer/scheduler stateë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ (ë””ìŠ¤í¬/I-O ì ˆì•½)")
    parser.add_argument("--bos_is_strategy", action="store_true",
                        help="ë””ì½”ë” prefixë¥¼ [STRAT][SYS] ìˆœì„œë¡œ ë‘˜ì§€ ì—¬ë¶€. ê¸°ë³¸ì€ [SYS][STRAT].")
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[logging.StreamHandler()],
    )
    set_seed(args.seed)

    # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì´ˆê¸°í™”
    tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
    tokenizer.truncation_side = "left"
    
    # íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens_dict = {"additional_special_tokens": list(SPECIAL_TOKENS.values())}
    num_added = tokenizer.add_special_tokens(special_tokens_dict)
    logger.info(f"{num_added} íŠ¹ìˆ˜ í† í° ì¶”ê°€ë¨")
    
    # í† í¬ë‚˜ì´ì € ì •ë³´ ì¶œë ¥
    logger.info(f"BOS token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}")
    logger.info(f"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}")
    logger.info(f"PAD token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}")
    
    # ëª¨ë¸ ë¡œë“œ ì‹œ ìƒì„± ê´€ë ¨ ì„¤ì •ì„ í¬í•¨í•˜ì§€ ì•Šë„ë¡ êµ¬ì„±
    model_config = BartConfig.from_pretrained("facebook/bart-base")
    # ìƒì„± ê´€ë ¨ ì„¤ì • ì œê±°
    for param in ['num_beams', 'max_length', 'early_stopping', 'no_repeat_ngram_size', 
                  'length_penalty', 'forced_bos_token_id', 'forced_eos_token_id']:
        if hasattr(model_config, param):
            delattr(model_config, param)
    
    model = BartForConditionalGeneration.from_pretrained(
        "facebook/bart-base", 
        config=model_config,
        ignore_mismatched_sizes=True
    )
    
    # ì„ë² ë”© í¬ê¸° í™•ì¥ (íŠ¹ìˆ˜ í† í° ìˆ˜ìš©)
    model.resize_token_embeddings(len(tokenizer))
    
    # ìƒì„± ì„¤ì • êµ¬ì„± 
    generation_config = GenerationConfig(
        max_length=args.max_tgt_length,
        min_length=5,
        num_beams=5,
        early_stopping=False,
        no_repeat_ngram_size=3,
        length_penalty=1.0,
        repetition_penalty=1.2,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )
    model.generation_config = generation_config

    # -------- dataset --------
    use_cache = not args.no_cache  # ìºì‹œ ì‚¬ìš© ì—¬ë¶€
    train_ds = DialogGenDataset(
        "train", tokenizer, args.max_src_length, args.max_tgt_length,
        tiny_frac=args.tiny_frac, dataset_name=args.dataset, use_cache=use_cache,
        bos_is_strategy=args.bos_is_strategy
    )
    val_ds = DialogGenDataset(
        "validation", tokenizer, args.max_src_length, args.max_tgt_length,
        tiny_frac=args.tiny_frac, dataset_name=args.dataset, use_cache=use_cache,
        bos_is_strategy=args.bos_is_strategy
    )
    test_ds = DialogGenDataset(
        "test", tokenizer, args.max_src_length, args.max_tgt_length,
        tiny_frac=args.tiny_frac, dataset_name=args.dataset, use_cache=use_cache,
        bos_is_strategy=args.bos_is_strategy
    )

    # ì»¤ìŠ¤í…€ ì½œë ˆì´í„° ì‚¬ìš©: decoder_input_ids ìœ ì§€ (prefix ë§ˆìŠ¤í‚¹ í•´ì œ)
    data_collator = CustomDataCollatorWithDecoderPrefix(
        tokenizer=tokenizer,
        model=None,  # shift ì‘ì—… ë¹„í™œì„±í™”
        padding="longest",
        decoder_prefix_len=0,  # prefixì—ë„ lossë¥¼ ì£¼ë¯€ë¡œ ë§ˆìŠ¤í‚¹í•˜ì§€ ì•ŠìŒ
    )

    # -------------------- Safe Trainer ì •ì˜ --------------------
    class SafeSeq2SeqTrainer(Seq2SeqTrainer):
        """RuntimeError ë°œìƒ ì‹œ optimizer state ì €ì¥ì„ ê±´ë„ˆë›°ëŠ” Trainer."""

        def _save_optimizer_and_scheduler(self, output_dir: str):  # type: ignore
            if args.no_save_optimizer:
                logger.info("âš ï¸  no_save_optimizer í”Œë˜ê·¸ê°€ ì„¤ì •ë˜ì–´ optimizer/scheduler state ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            try:
                return super()._save_optimizer_and_scheduler(output_dir)
            except RuntimeError as e:
                logger.warning(f"optimizer/scheduler ì €ì¥ ì‹¤íŒ¨: {e}. í•´ë‹¹ ìŠ¤í…ì—ì„œ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                torch.cuda.empty_cache()
                return

    # -------------------- Prefix-aware Trainer --------------------
    class PrefixSeq2SeqTrainer(SafeSeq2SeqTrainer):
        """Seq2SeqTrainer í™•ì¥: generate ì‹œ decoder prefix ì„¸ í† í°(</s> [STRAT] [SYS] ë˜ëŠ” </s> [SYS] [STRAT])ì„ ì‚¬ìš©"""

        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
            if not self.args.predict_with_generate or prediction_loss_only:
                return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)

            # prefix ì„¸ í† í° ì¶”ì¶œ (íŒ¨ë”© ì œì™¸)
            decoder_prefix = inputs["decoder_input_ids"][:, :3].to(model.device)  # ë³€ê²½: prefix ê¸¸ì´ 3

            # ìƒì„± ì‹œ decoder_input_ids ë¡œê¹… (ì²« ì˜ˆì¸¡ì—ì„œë§Œ)
            if self.state.global_step < 10 or self.state.global_step % 100 == 0:
                logger.info("\n===== ìƒì„± ì…ë ¥ ë””ë²„ê¹… =====")
                logger.info(f"decoder_prefix í˜•íƒœ: {decoder_prefix.shape}")
                logger.info(f"ì²« ìƒ˜í”Œ decoder_prefix: {decoder_prefix[0].tolist()}")
                
                # ë””ì½”ë”©í•´ì„œ í™•ì¸
                prefix_text = tokenizer.decode(decoder_prefix[0], skip_special_tokens=False)
                logger.info(f"ì²« ìƒ˜í”Œ prefix ë””ì½”ë”©: '{prefix_text}'")

            gen_kwargs = {
                "max_length": self.args.generation_max_length or model.generation_config.max_length,
                "num_beams": self.args.generation_num_beams or model.generation_config.num_beams,
                "decoder_input_ids": decoder_prefix,
            }

            # ìƒì„± ì§ì „ ìµœì¢… í™•ì¸
            logger.info(f"ìƒì„± kwargs: {gen_kwargs}")

            generated_tokens = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                **gen_kwargs,
            )

            # padding output length
            if generated_tokens.shape[-1] < gen_kwargs["max_length"]:
                pad_length = gen_kwargs["max_length"] - generated_tokens.shape[-1]
                pad_tensor = torch.full((generated_tokens.shape[0], pad_length), model.generation_config.pad_token_id, dtype=generated_tokens.dtype, device=generated_tokens.device)
                generated_tokens = torch.cat([generated_tokens, pad_tensor], dim=-1)

            loss = None
            if "labels" in inputs:
                with torch.no_grad():
                    outputs = model(**inputs)
                    loss = outputs.loss.detach()

            return loss, generated_tokens, inputs.get("labels")

        def training_step(self, model, inputs, num_items_in_batch=None):
            """í•™ìŠµ ìŠ¤í… ì˜¤ë²„ë¼ì´ë“œ: decoder_input_idsì™€ labels ë¡œê¹…"""
            # ì²« ë°°ì¹˜ì˜ ì²« ìƒ˜í”Œë§Œ ë¡œê¹… (ê³¼ë„í•œ ì¶œë ¥ ë°©ì§€)
            if self.state.global_step == 0:
                logger.info("\n===== í•™ìŠµ ì…ë ¥ ë””ë²„ê¹… =====")
                logger.info(f"decoder_input_ids ìƒ˜í”Œ: {inputs['decoder_input_ids'][0][:10].tolist()}")
                logger.info(f"labels ìƒ˜í”Œ: {inputs['labels'][0][:10].tolist()}")
                
                # ì²« ìƒ˜í”Œë¡œ ì§ì ‘ forward í˜¸ì¶œ í…ŒìŠ¤íŠ¸
                logger.info("\n===== ì§ì ‘ ëª¨ë¸ í˜¸ì¶œ í…ŒìŠ¤íŠ¸ =====")
                test_inputs = {
                    "input_ids": inputs["input_ids"][0:1],
                    "attention_mask": inputs["attention_mask"][0:1],
                    "decoder_input_ids": inputs["decoder_input_ids"][0:1],
                    "labels": inputs["labels"][0:1]
                }
                with torch.no_grad():
                    outputs = model(**test_inputs)
                logger.info(f"ì§ì ‘ í˜¸ì¶œ loss: {outputs.loss.item()}")
                
                # ë””ì½”ë”©í•´ì„œ í™•ì¸
                dec_in = tokenizer.decode(inputs["decoder_input_ids"][0], skip_special_tokens=False)
                labels_masked = inputs["labels"][0].clone()
                labels_masked[labels_masked == -100] = tokenizer.pad_token_id
                labels_txt = tokenizer.decode(labels_masked, skip_special_tokens=False)
                logger.info(f"ë””ì½”ë” ì…ë ¥ ë””ì½”ë”©: {dec_in[:100]}")
                logger.info(f"ë¼ë²¨ ë””ì½”ë”©: {labels_txt[:100]}")
            
            return super().training_step(model, inputs, num_items_in_batch)

    # -------- training args --------
    t_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=0.01,
        warmup_ratio=args.warmup_ratio,
        eval_strategy="steps",
        eval_steps=args.eval_steps,
        save_strategy="steps",
        save_steps=args.eval_steps,
        save_total_limit=1,
        logging_strategy="steps",
        logging_steps=args.eval_steps,
        predict_with_generate=True,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,  # lossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        report_to="none",
        seed=args.seed,
        # ë©”ëª¨ë¦¬ ì²´í¬ ë¹„í™œì„±í™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
        dataloader_drop_last=False,
        skip_memory_metrics=True,
    )
    
    # ì‹¤ì œ ë°°ì¹˜ í¬ê¸° ë¡œê·¸ ì¶œë ¥
    effective_batch_size = args.batch_size * args.gradient_accumulation_steps
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„: {args.gradient_accumulation_steps}")
    logger.info(f"ì‹¤íš¨ ë°°ì¹˜ í¬ê¸°: {effective_batch_size} (batch_size * gradient_accumulation_steps)")

    # ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜
    def compute_metrics(eval_pred):
        preds, labels = eval_pred
        labels[labels == -100] = tokenizer.pad_token_id

        # ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        metrics = {}

        # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        gen_raw = tokenizer.batch_decode(preds, skip_special_tokens=True)
        ref_raw = tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        # MultiESC ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬: ì†Œë¬¸ìí™” ë° nltk í† í°í™”
        gen_txt = []
        ref_txt = []
        for gen, ref in zip(gen_raw, ref_raw):
            # ì†Œë¬¸ìí™” í›„ nltk í† í°í™”í•˜ì—¬ ê³µë°±ìœ¼ë¡œ ì¬ê²°í•©
            gen_processed = ' '.join(nltk.word_tokenize(gen.lower()))
            ref_processed = ' '.join(nltk.word_tokenize(ref.lower()))
            gen_txt.append(gen_processed)
            ref_txt.append(ref_processed)

        # ìƒì„± ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU, ROUGE ë“±)
        text_metrics = generation_metrics(gen_txt, ref_txt)
        
        # í‰ê°€ ë©”íŠ¸ë¦­ì— "eval_" ì ‘ë‘ì‚¬ ì¶”ê°€
        for k, v in text_metrics.items():
            metrics[f'eval_{k}'] = v

        return metrics

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê´€ë ¨ ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì½œë°±
    class TokenEmbeddingCallback(TrainerCallback):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œ í† í° ì„ë² ë”© í¬ê¸°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ìœ ì§€"""
        def __init__(self, tokenizer, special_token_ids):
            self.tokenizer = tokenizer
            self.special_token_ids = special_token_ids
        
        def on_train_begin(self, args, state, control, model=None, **kwargs):
            """í›ˆë ¨ ì‹œì‘ ì‹œ ëª¨ë¸ ì„ë² ë”© í¬ê¸° í™•ì¸"""
            if model is not None:
                model.resize_token_embeddings(len(self.tokenizer))
                logger.info(f"ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì •: {len(self.tokenizer)}")
        
        def on_step_end(self, args, state, control, model=None, **kwargs):
            """ê° ìŠ¤í… í›„ ì„ë² ë”© í¬ê¸° í™•ì¸"""
            if state.global_step % 1000 == 0 and model is not None:
                if model.get_input_embeddings().weight.shape[0] != len(self.tokenizer):
                    logger.warning(f"ì„ë² ë”© í¬ê¸° ë¶ˆì¼ì¹˜ ê°ì§€: {model.get_input_embeddings().weight.shape[0]} vs {len(self.tokenizer)}")
                    model.resize_token_embeddings(len(self.tokenizer))
        
        def on_load_checkpoint(self, args, state, control, **kwargs):
            """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œ í˜¸ì¶œ"""
            logger.info("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘...")
            
        def on_checkpoint_model_loading(self, args, state, control, model=None, **kwargs):
            """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œ í˜¸ì¶œ"""
            if model is not None:
                # ì„ë² ë”© í¬ê¸° ì¡°ì •
                model.resize_token_embeddings(len(self.tokenizer))
                logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í›„ ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì •: {len(self.tokenizer)}")
                
                # ìƒì„± ì„¤ì • ë‹¤ì‹œ ì ìš©
                model.generation_config = generation_config
                
                # ë¯¸ë¦¬ êµ¬ì„±ëœ ëª¨ë¸ë¡œ ì´ˆê¸° ê°€ì¤‘ì¹˜ ë³µì‚¬ (ì„ íƒ ì‚¬í•­)
                if hasattr(model, 'lm_head') and not hasattr(model.lm_head, 'weight'):
                    logger.warning("lm_headì— weightê°€ ì—†ìŒ, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í•„ìš”")
                    # í•„ìš”í•œ ê²½ìš° ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë¡œì§ ì¶”ê°€

    # íŠ¹ìˆ˜ í† í° ID ëª©ë¡
    special_token_ids = [
        tokenizer.bos_token_id, 
        tokenizer.eos_token_id, 
        tokenizer.pad_token_id,
        *[tokenizer.convert_tokens_to_ids(t) for t in SPECIAL_TOKENS.values()]
    ]
    
    # ìµœì¢… ì½œë°± ëª©ë¡ ìƒì„±
    callbacks = [
        EarlyStoppingCallback(
            early_stopping_patience=args.patience,
            early_stopping_threshold=0.0
        ),
        TokenEmbeddingCallback(tokenizer, special_token_ids)
    ]

    trainer = PrefixSeq2SeqTrainer(
        model=model,  # ì´ˆê¸° ëª¨ë¸ (í•™ìŠµ ì „ìš©)
        args=t_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=callbacks,
    )

    # --------------------- ì´ˆê¸° ëª¨ë¸(epoch 0) í‰ê°€ ---------------------
    if args.eval_init or args.eval_only:
        logger.info("ğŸ“Š Evaluating initial model (epoch 0) ...")
        
        # ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€
        init_eval = trainer.evaluate()
        
        # ê²°ê³¼ ì €ì¥
        init_path = Path(args.output_dir) / "init_eval_metrics.json"
        init_path.parent.mkdir(exist_ok=True, parents=True)
        with init_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) if isinstance(v, (int, float, np.number)) else v 
                       for k, v in init_eval.items()}, f, indent=2)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€
        logger.info("ğŸ“Š Evaluating initial model on test split ...")
        
        
        # í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
        test_out = trainer.predict(test_ds, metric_key_prefix="init_test")
        
        # PPL ê³„ì‚°
        test_ppl = float(np.exp(test_out.metrics.get('init_test_loss', 0)))
        logger.info(f"Initial Test Perplexity: {test_ppl:.4f}")
        
        # ìƒ˜í”Œ ì €ì¥ìš© ì›ë³¸ í…ìŠ¤íŠ¸ (MultiESC ì „ì²˜ë¦¬ ì „)
        lbl_ids = test_out.label_ids.copy()
        lbl_ids[lbl_ids == -100] = tokenizer.pad_token_id
        
        gen_raw = tokenizer.batch_decode(test_out.predictions, skip_special_tokens=True)
        ref_raw = tokenizer.batch_decode(lbl_ids, skip_special_tokens=True)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°ìš© í…ìŠ¤íŠ¸ (MultiESC ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬)
        gen_txt = []
        ref_txt = []
        for gen, ref in zip(gen_raw, ref_raw):
            gen_processed = ' '.join(nltk.word_tokenize(gen.lower()))
            ref_processed = ' '.join(nltk.word_tokenize(ref.lower()))
            gen_txt.append(gen_processed)
            ref_txt.append(ref_processed)
            
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        text_metrics = generation_metrics(gen_txt, ref_txt)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        init_test_metrics = {}
        for k, v in text_metrics.items():
            init_test_metrics[f'init_test_{k}'] = v
        init_test_metrics.update(test_out.metrics)
        init_test_metrics['init_test_perplexity'] = test_ppl
        
        # ê²°ê³¼ ì €ì¥
        init_test_path = Path(args.output_dir) / "init_test_metrics.json"
        with init_test_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) if isinstance(v, (int, float, np.number)) else v 
                       for k, v in init_test_metrics.items()}, f, indent=2)
        
        # ìƒ˜í”Œ ì €ì¥ (ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ì €ì¥)
        sample_n = min(10, len(gen_raw))
        with open(Path(args.output_dir) / "init_samples.txt", "w", encoding="utf-8") as f:
            for i, (ref, gen) in enumerate(zip(ref_raw[:sample_n], gen_raw[:sample_n])):
                context = test_ds.examples[i]["context"]
                f.write(f"CONTEXT: {context}\nREF: {ref}\nGEN: {gen}\n---\n")
        
        logger.info(f"ğŸ“ Saved initial model metrics to {args.output_dir}")
    
    # í•™ìŠµ ìˆ˜í–‰ (eval_onlyê°€ Trueë©´ í•™ìŠµ ê±´ë„ˆëœ€)
    if not args.eval_only:
        trainer.train()
        
        # í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥ - ì¤‘ìš”: ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ì €ì¥
        # safe_serialization=TrueëŠ” ë¯¸ë˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì˜µì…˜
        model_path = Path(args.output_dir)
        model.save_pretrained(model_path, safe_serialization=True)
        tokenizer.save_pretrained(model_path)
        
        logger.info(f"Model explicitly saved to {args.output_dir}")
        
        # --------------------- test split í‰ê°€ ---------------------
        logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ ì¤‘...")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
        test_out = trainer.predict(test_ds, metric_key_prefix="test")

        # PPL ê³„ì‚°
        test_ppl = float(np.exp(test_out.metrics.get('test_loss', 0)))
        logger.info(f"Test Perplexity: {test_ppl:.4f}")

        # ìƒ˜í”Œ ì €ì¥ìš© ì›ë³¸ í…ìŠ¤íŠ¸ (MultiESC ì „ì²˜ë¦¬ ì „)
        lbl_ids = test_out.label_ids.copy()
        lbl_ids[lbl_ids == -100] = tokenizer.pad_token_id
        
        gen_raw = tokenizer.batch_decode(test_out.predictions, skip_special_tokens=True)
        ref_raw = tokenizer.batch_decode(lbl_ids, skip_special_tokens=True)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°ìš© í…ìŠ¤íŠ¸ (MultiESC ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬)
        gen_txt = []
        ref_txt = []
        for gen, ref in zip(gen_raw, ref_raw):
            gen_processed = ' '.join(nltk.word_tokenize(gen.lower()))
            ref_processed = ' '.join(nltk.word_tokenize(ref.lower()))
            gen_txt.append(gen_processed)
            ref_txt.append(ref_processed)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        text_metrics = generation_metrics(gen_txt, ref_txt)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        test_metrics = {}
        for k, v in text_metrics.items():
            test_metrics[f'test_{k}'] = v
        test_metrics.update(test_out.metrics)
        test_metrics['test_perplexity'] = test_ppl
        
        with open(Path(args.output_dir) / "test_metrics.json", "w") as f:
            json.dump({k: float(v) if isinstance(v, (int, float, np.number)) else v 
                       for k, v in test_metrics.items()}, f, indent=2)

        # ìƒ˜í”Œ ì €ì¥ (ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ì €ì¥)
        sample_n = min(20, len(gen_raw))
        with open(Path(args.output_dir) / "test_samples.txt", "w", encoding="utf-8") as f:
            for i, (ref, gen) in enumerate(zip(ref_raw[:sample_n], gen_raw[:sample_n])):
                context = test_ds.examples[i]["context"]
                f.write(f"CONTEXT: {context}\nREF: {ref}\nGEN: {gen}\n---\n")

        logger.info(f"ëª¨ë¸ ë° í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì €ì¥ ì™„ë£Œ: {args.output_dir}")

    if args.show_samples:
        import textwrap
        # í† í¬ë‚˜ì´ì €ë¡œ í™•ì¸í•˜ëŠ” ìƒ˜í”Œ ë°ì´í„°
        for i in random.sample(range(len(train_ds)), args.show_samples):
            ex = train_ds[i]

            ctx_plain = safe_decode(ex["input_ids"], tokenizer, skip_special_tokens=False)
            tgt_plain = safe_decode(ex["labels"], tokenizer, skip_special_tokens=False)
            
            logging.info(
                "\n===== SAMPLE {:d} =====\n"
                "CONTEXT:\n{}\n\n"
                "TARGET:\n{}\n"
                "{}".format(
                    i,
                    ctx_plain, 
                    tgt_plain,
                    "=" * 60
                )
            )
        
        # ì‹¤ì œ ìƒì„± í…ŒìŠ¤íŠ¸
        logger.info("\n===== ìƒì„± ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ =====")
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
        model.eval()
        
        # ëª‡ ê°œì˜ ëœë¤ ìƒ˜í”Œë¡œ ìƒì„± í…ŒìŠ¤íŠ¸
        for i in random.sample(range(len(val_ds)), min(3, len(val_ds))):
            ex = val_ds[i]
            input_ids = torch.tensor([ex["input_ids"]]).to(model.device)
            attention_mask = torch.tensor([ex["attention_mask"]]).to(model.device)
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                # ë””ì½”ë” prefix ë¡œê¹…
                decoder_prefix = torch.tensor([ex["decoder_input_ids"][:3]]).to(model.device)
                logger.info(f"\nìƒ˜í”Œ ìƒì„± í…ŒìŠ¤íŠ¸ - decoder_prefix: {decoder_prefix.tolist()}")
                logger.info(f"ë””ì½”ë”©ëœ prefix: '{tokenizer.decode(decoder_prefix[0], skip_special_tokens=False)}'")
                
                outputs = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    decoder_input_ids=decoder_prefix,
                    max_length=args.max_tgt_length,
                )
            
            # ë””ì½”ë”© ë° ì¶œë ¥ - pad í† í°ë§Œ ì œê±°í•˜ê³  íŠ¹ìˆ˜ í† í°ì€ ìœ ì§€
            skip_special_tokens = True

            generated_text = safe_decode(outputs[0].tolist(), tokenizer, skip_special_tokens=skip_special_tokens)
            target_text = safe_decode(ex["labels"], tokenizer, skip_special_tokens=skip_special_tokens)
            context_text = safe_decode(ex["input_ids"], tokenizer, skip_special_tokens=skip_special_tokens)
            
            logger.info(
                "\n----- ìƒì„± ìƒ˜í”Œ {:d} -----\n"
                "ì…ë ¥ ë¬¸ë§¥:\n{}\n\n"
                "ì •ë‹µ ì‘ë‹µ:\n{}\n\n"
                "ìƒì„± ì‘ë‹µ:\n{}\n"
                "{}".format(
                    i,
                    context_text,  
                    target_text,   
                    generated_text,
                    "-" * 60
                )
            )
               
        return


if __name__ == "__main__":
    main() 