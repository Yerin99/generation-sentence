# -*- coding: utf-8 -*-
"""
bart_dialog_generator_cls_prefix.py
============================
BART ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ë§¥ë½ê³¼ ì „ëµ(strategy)ì„ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” íŒŒì´í”„ë¼ì¸.

ì‚¬ìš©ë²• ì˜ˆì‹œ:
----------
# ê¸°ë³¸ í›ˆë ¨
CUDA_VISIBLE_DEVICES=1 python bart_dialog_generator_cls_prefix.py --batch_size 16 --output_dir outputs/dialog_generation_cls_prefix

# ì‘ì€ ë¹„ìœ¨ì˜ ë°ì´í„°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
CUDA_VISIBLE_DEVICES=1 python bart_dialog_generator_cls_prefix.py --tiny_frac 0.05 --epochs 1 --eval_steps 10 --output_dir outputs/dialog_tiny_cls_prefix

# facebook/bart-base ì›ë³¸ ëª¨ë¸ í‰ê°€
CUDA_VISIBLE_DEVICES=2 python bart_dialog_generator_cls_prefix.py --eval_only --output_dir outputs/dialog_eval_cls_prefix

# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ì‚¬ìš©í•œ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ í•™ìŠµ (ì‹¤íš¨ ë°°ì¹˜ í¬ê¸° 32)
CUDA_VISIBLE_DEVICES=3 python bart_dialog_generator_cls_prefix.py --gradient_accumulation_steps 2 --output_dir outputs/dialog_batch_32_cls_prefix
"""

from __future__ import annotations

import argparse, json, logging, random
from pathlib import Path
from typing import Dict, List

import numpy as np
import torch
import nltk
from datasets import load_dataset
from transformers import (
    BartTokenizer,
    BartForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback,
    TrainerCallback,
    GenerationConfig,
    set_seed,
    BartConfig,
)
from utils.metrics import generation_metrics

# ===================== ì „ì—­ ì •ì˜ =====================
SPECIAL_TOKENS = {         
    "usr": "[USR]",
    "sys": "[SYS]",
    "Question": "[STRAT_Question]",
    "Restatement or Paraphrasing": "[STRAT_Paraphrasing]",
    "Reflection of feelings": "[STRAT_Reflection]",
    "Self-disclosure": "[STRAT_SelfDisclosure]",
    "Affirmation and Reassurance": "[STRAT_Reassurance]",
    "Providing Suggestions": "[STRAT_Suggestion]",
    "Information": "[STRAT_Information]",
    "Others": "[STRAT_Others]",
}

# ì „ëµ ì´ë¦„ ê³ ì • ìˆœì„œ (ë¶„ë¥˜ê¸°ì™€ ë™ì¼í•˜ê²Œ 8ê°œ)
STRATEGIES = [
    "Question",
    "Restatement or Paraphrasing",
    "Reflection of feelings",
    "Self-disclosure",
    "Affirmation and Reassurance",
    "Providing Suggestions",
    "Information",
    "Others",
]

logger = logging.getLogger("dialog_gen")

# ===================== ë°ì´í„°ì…‹ =====================
class DialogGenDataset(torch.utils.data.Dataset):
    """
    ëŒ€í™” ìƒì„±ì„ ìœ„í•œ ë°ì´í„°ì…‹.
    
    ëŒ€í™” ë§¥ë½(context)ì„ ì…ë ¥ìœ¼ë¡œ, ì‹œìŠ¤í…œ ì‘ë‹µì„ ì¶œë ¥ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°ì…‹
    """

    def __init__(
        self,
        split: str,
        tokenizer: BartTokenizer,
        max_src: int = 512,
        max_tgt: int = 128,
        tiny_frac: float | None = None,
        cache_dir: str = "cache_dialog_gen_cls_prefix",
        dataset_name: str = "thu-coai/esconv",
        use_cache: bool = True,  # ìºì‹œ ì‚¬ìš© ì—¬ë¶€ ì„ íƒ ì˜µì…˜
    ):
        self.tok = tokenizer
        self.max_src, self.max_tgt = max_src, max_tgt
        self.dataset_name = dataset_name

        # ë°ì´í„°ì…‹ ë¡œë“œ
        raw = load_dataset(dataset_name, split=split)

        # ì‘ì€ ë¹„ìœ¨ë§Œ ì‚¬ìš© (ë””ë²„ê¹…ìš©)
        if tiny_frac:
            raw = raw.shuffle(seed=42).select(range(int(len(raw) * tiny_frac)))

        cache_f = (
            Path(cache_dir)
            / f"{split}_{max_src}_{max_tgt}_{tiny_frac}_{dataset_name.replace('/', '_')}.pt"
        )
        # ìºì‹œ ì‚¬ìš©ì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ìºì‹œ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
        if use_cache and cache_f.exists():
            # ë³´ì•ˆ ê²½ê³  í•´ê²°: torch.save/load ì‹œ ê°ì²´ ì§ë ¬í™” ëŒ€ì‹  pickle ì‚¬ìš©
            import pickle
            with open(cache_f, 'rb') as f:
                self.examples = pickle.load(f)
            return

        self.examples: List[Dict] = []
        for ex in raw:
            dialog = json.loads(ex["text"])["dialog"]
            for turn_idx, turn in enumerate(dialog):
                if turn["speaker"] != "sys":
                    continue
                
                # 1) contextë¥¼ </s>ë¡œ êµ¬ë¶„
                ctx_parts = []
                for prev in dialog[:turn_idx]:
                    spk_tok = SPECIAL_TOKENS["usr"] if prev["speaker"] == "usr" else SPECIAL_TOKENS["sys"]
                    if prev["speaker"] == "usr":
                        ctx_parts.append(f"{spk_tok}{prev['text']}")
                    else:
                        strategy = prev["strategy"]
                        ctx_parts.append(f"{spk_tok}{SPECIAL_TOKENS[strategy]}{prev['text']}")

                # íŠ¹ìˆ˜ í† í°ì„ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ í† í¬ë‚˜ì´ì €ì˜ bos/eos í† í°ìœ¼ë¡œ ì‚¬ìš©
                context = tokenizer.bos_token + (tokenizer.eos_token.join(ctx_parts) if ctx_parts else "") + tokenizer.eos_token

                if not context.strip():
                    continue

                # ---------- decoder output (response) ----------
                # ë””ì½”ë” prefix: [SYS]  [STRAT_x]  + ì‹¤ì œ ì‘ë‹µ
                tgt_text = SPECIAL_TOKENS["sys"] + SPECIAL_TOKENS[turn["strategy"]]+ turn["text"] + tokenizer.eos_token

                # 2) add_special_tokens=Falseë¡œ í† í¬ë‚˜ì´ì¦ˆ
                enc = self.tok(
                    context,
                    max_length=self.max_src,
                    truncation=True,
                    padding="max_length",
                    add_special_tokens=False
                )
                dec = self.tok(
                    tgt_text,
                    max_length=self.max_tgt,
                    truncation=True,
                    padding="max_length",
                    add_special_tokens=False
                )

                # label -100 masking
                #  - pad í† í°
                #  - ì „ëµ í† í°(ë””ì½”ë” prefix, index 1)
                labels = []
                for idx, tid in enumerate(dec.input_ids):
                    if tid == self.tok.pad_token_id or idx == 1:  # idx==1 -> strategy token
                        labels.append(-100)
                    else:
                        labels.append(tid)

                # ì „ëµ id (0~7)
                strat_id = STRATEGIES.index(turn["strategy"])

                self.examples.append({
                    "input_ids": enc.input_ids,
                    "attention_mask": enc.attention_mask,
                    "labels": labels,
                    "strategy_id": strat_id,
                    "context": context,
                    "response": tgt_text,
                })
        # ìºì‹œ ì‚¬ìš©ì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ìºì‹œ íŒŒì¼ ì €ì¥
        if use_cache:
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            cache_f.parent.mkdir(exist_ok=True, parents=True)
            # ë³´ì•ˆ ê²½ê³  í•´ê²°: torch.save/load ì‹œ ê°ì²´ ì§ë ¬í™” ëŒ€ì‹  pickle ì‚¬ìš©
            import pickle
            with open(cache_f, 'wb') as f:
                pickle.dump(self.examples, f)

    # -------------- torch Dataset interface --------------
    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        # ì´ë¯¸ í† í°í™”ëœ ì˜ˆì œë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return self.examples[idx]


# ===================== ìœ í‹¸ í•¨ìˆ˜ =====================
def safe_decode(ids, tokenizer, skip_special_tokens=False, **kwargs):
    """ì•ˆì „í•˜ê²Œ ë””ì½”ë”©í•˜ë˜, pad í† í°ë§Œ ì œì™¸í•˜ê³  ë‹¤ë¥¸ special tokenì€ ìœ ì§€"""
    try:
        # ë¨¼ì € pad í† í°ì˜ ìœ„ì¹˜ ì°¾ê¸°
        if isinstance(ids, list):
            # pad í† í°ì´ ì‹œì‘ë˜ëŠ” ì²« ìœ„ì¹˜ ì°¾ê¸°
            pad_positions = [i for i, id in enumerate(ids) if id == tokenizer.pad_token_id]
            # pad í† í°ì´ ìˆìœ¼ë©´ ì²« pad í† í° ì „ê¹Œì§€ë§Œ ì‚¬ìš©
            ids_without_pad = ids[:pad_positions[0]] if pad_positions else ids
            # ë””ì½”ë”© ì‹œ íŠ¹ìˆ˜ í† í° ìœ ì§€
            return tokenizer.decode(ids_without_pad, skip_special_tokens=skip_special_tokens, **kwargs)
        else:
            return tokenizer.decode(ids, skip_special_tokens=skip_special_tokens, **kwargs)
    except Exception as e:
        logger.warning(f"ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        # ìŒìˆ˜ ë° ë²”ìœ„ ì´ˆê³¼ ID ì œê±°
        valid_ids = [i for i in ids if i >= 0 and i < len(tokenizer)]
        return tokenizer.decode(valid_ids, skip_special_tokens=skip_special_tokens, **kwargs)


# ===================== ë©”ì¸ =====================
def main():
    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_dir", type=str, default="outputs/dialog_gen")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1,
                        help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜ (ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = batch_size * gradient_accumulation_steps)")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--tiny_frac", type=float, default=None, help="0~1: ë””ë²„ê·¸ìš© ìƒ˜í”Œ ë¹„ìœ¨")
    parser.add_argument("--patience", type=int, default=5,
                        help="early stopping patience (eval_loss ê¸°ì¤€)")
    parser.add_argument("--eval_steps", type=int, default=500,
                        help="evaluation interval (steps)")
    parser.add_argument("--show_samples", type=int, default=10,
                        help="ìƒ˜í”Œ nê°œ(context/target) ì¶œë ¥ í›„ ì¢…ë£Œ")
    parser.add_argument("--eval_init", action="store_true", 
                        help="í•™ìŠµ ì „ ì´ˆê¸° ëª¨ë¸(epoch 0)ì—ì„œ í‰ê°€ ìˆ˜í–‰")
    parser.add_argument("--eval_only", action="store_true",
                        help="í•™ìŠµ ì—†ì´ í‰ê°€ë§Œ ìˆ˜í–‰")
    parser.add_argument("--dataset", type=str, default="thu-coai/esconv",
                        help="ì‚¬ìš©í•  ë°ì´í„°ì…‹ (ê¸°ë³¸ê°’: ESConv)")
    parser.add_argument("--learning_rate", type=float, default=3e-5,
                        help="í•™ìŠµë¥  (ê¸°ë³¸ê°’: 3e-5, í° ë°ì´í„°ì…‹ì€ 5e-5ë„ ê°€ëŠ¥)")
    parser.add_argument("--max_src_length", type=int, default=896,
                        help="ìµœëŒ€ ì†ŒìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 896)")
    parser.add_argument("--max_tgt_length", type=int, default=256,
                        help="ìµœëŒ€ íƒ€ê²Ÿ ê¸¸ì´")
    parser.add_argument("--warmup_ratio", type=float, default=0.05,
                        help="warmup ë¹„ìœ¨ (ì „ì²´ ìŠ¤í…ì˜ %, ì´ˆê¸° ë¶ˆì•ˆì •í•œ loss spike ë°©ì§€)")
    parser.add_argument("--num_samples", type=int, default=5, 
                        help="í•™ìŠµ/í‰ê°€ ì¤‘ ì¶œë ¥í•  ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--no_cache", action="store_true",
                        help="ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•­ìƒ ë°ì´í„°ë¥¼ ìƒˆë¡œ ì²˜ë¦¬")
    parser.add_argument("--alpha", type=float, default=0.5,
                        help="ì „ëµ loss ê°€ì¤‘ì¹˜ (0~1, ì „ì²´ loss = gen_loss + alpha * strat_loss)")
    parser.add_argument("--label_smoothing_factor", type=float, default=0.1,
                        help="generation loss label smoothing ê³„ìˆ˜")
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[logging.StreamHandler()],
    )
    set_seed(args.seed)

    # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì´ˆê¸°í™”
    tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
    tokenizer.truncation_side = "left"
    
    # íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens_dict = {"additional_special_tokens": list(SPECIAL_TOKENS.values())}
    num_added = tokenizer.add_special_tokens(special_tokens_dict)
    logger.info(f"{num_added} íŠ¹ìˆ˜ í† í° ì¶”ê°€ë¨")
    
    # í† í¬ë‚˜ì´ì € ì •ë³´ ì¶œë ¥
    logger.info(f"BOS token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}")
    logger.info(f"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}")
    logger.info(f"PAD token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}")
    
    # ëª¨ë¸ ë¡œë“œ ì‹œ ìƒì„± ê´€ë ¨ ì„¤ì •ì„ í¬í•¨í•˜ì§€ ì•Šë„ë¡ êµ¬ì„±
    model_config = BartConfig.from_pretrained("facebook/bart-base")
    # ìƒì„± ê´€ë ¨ ì„¤ì • ì œê±°
    for param in ['num_beams', 'max_length', 'early_stopping', 'no_repeat_ngram_size', 
                  'length_penalty', 'forced_bos_token_id', 'forced_eos_token_id']:
        if hasattr(model_config, param):
            delattr(model_config, param)
    
    model = BartForConditionalGeneration.from_pretrained(
        "facebook/bart-base", 
        config=model_config,
        ignore_mismatched_sizes=True
    )
    
    # ì„ë² ë”© í¬ê¸° í™•ì¥ (íŠ¹ìˆ˜ í† í° ìˆ˜ìš©)
    model.resize_token_embeddings(len(tokenizer))
    
    # ìƒì„± ì„¤ì • êµ¬ì„± 
    generation_config = GenerationConfig(
        max_length=args.max_tgt_length,
        num_beams=5,
        early_stopping=False,
        no_repeat_ngram_size=3,
        length_penalty=1.0,
        repetition_penalty=1.2,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )
    model.generation_config = generation_config

    # -------- dataset --------
    use_cache = not args.no_cache  # ìºì‹œ ì‚¬ìš© ì—¬ë¶€
    train_ds = DialogGenDataset(
        "train", tokenizer, args.max_src_length, args.max_tgt_length,
        tiny_frac=args.tiny_frac, dataset_name=args.dataset, use_cache=use_cache
    )
    val_ds = DialogGenDataset(
        "validation", tokenizer, args.max_src_length, args.max_tgt_length,
        tiny_frac=args.tiny_frac, dataset_name=args.dataset, use_cache=use_cache
    )
    test_ds = DialogGenDataset(
        "test", tokenizer, args.max_src_length, args.max_tgt_length,
        tiny_frac=args.tiny_frac, dataset_name=args.dataset, use_cache=use_cache
    )

    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding="longest")

    # -------- strategy classification head (ë©€í‹°íƒœìŠ¤í¬) --------
    import torch.nn as nn
    strategy_head = nn.Linear(model.config.d_model, len(STRATEGIES), bias=True).to(model.device)

    # -------- training args --------
    t_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=0.01,
        warmup_ratio=args.warmup_ratio,
        eval_strategy="steps",
        eval_steps=args.eval_steps,
        save_strategy="steps",
        save_steps=args.eval_steps,
        save_total_limit=1,
        logging_strategy="steps",
        logging_steps=args.eval_steps,
        predict_with_generate=True,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,  # lossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        report_to="none",
        seed=args.seed,
        # ë©”ëª¨ë¦¬ ì²´í¬ ë¹„í™œì„±í™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
        dataloader_drop_last=False,
        skip_memory_metrics=True,
        label_smoothing_factor=args.label_smoothing_factor,
    )
    
    # ì‹¤ì œ ë°°ì¹˜ í¬ê¸° ë¡œê·¸ ì¶œë ¥
    effective_batch_size = args.batch_size * args.gradient_accumulation_steps
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„: {args.gradient_accumulation_steps}")
    logger.info(f"ì‹¤íš¨ ë°°ì¹˜ í¬ê¸°: {effective_batch_size} (batch_size * gradient_accumulation_steps)")

    # ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜
    def compute_metrics(eval_pred):
        preds, labels = eval_pred
        labels[labels == -100] = tokenizer.pad_token_id

        # ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        metrics = {}

        # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        gen_raw = tokenizer.batch_decode(preds, skip_special_tokens=True)
        ref_raw = tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        # MultiESC ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬: ì†Œë¬¸ìí™” ë° nltk í† í°í™”
        gen_txt = []
        ref_txt = []
        for gen, ref in zip(gen_raw, ref_raw):
            # ì†Œë¬¸ìí™” í›„ nltk í† í°í™”í•˜ì—¬ ê³µë°±ìœ¼ë¡œ ì¬ê²°í•©
            gen_processed = ' '.join(nltk.word_tokenize(gen.lower()))
            ref_processed = ' '.join(nltk.word_tokenize(ref.lower()))
            gen_txt.append(gen_processed)
            ref_txt.append(ref_processed)

        # ìƒì„± ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU, ROUGE ë“±)
        text_metrics = generation_metrics(gen_txt, ref_txt)
        
        # í‰ê°€ ë©”íŠ¸ë¦­ì— "eval_" ì ‘ë‘ì‚¬ ì¶”ê°€
        for k, v in text_metrics.items():
            metrics[f'eval_{k}'] = v

        return metrics

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ê´€ë ¨ ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì½œë°±
    class TokenEmbeddingCallback(TrainerCallback):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œ í† í° ì„ë² ë”© í¬ê¸°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ìœ ì§€"""
        def __init__(self, tokenizer, special_token_ids):
            self.tokenizer = tokenizer
            self.special_token_ids = special_token_ids
        
        def on_train_begin(self, args, state, control, model=None, **kwargs):
            """í›ˆë ¨ ì‹œì‘ ì‹œ ëª¨ë¸ ì„ë² ë”© í¬ê¸° í™•ì¸"""
            if model is not None:
                model.resize_token_embeddings(len(self.tokenizer))
                logger.info(f"ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì •: {len(self.tokenizer)}")
        
        def on_step_end(self, args, state, control, model=None, **kwargs):
            """ê° ìŠ¤í… í›„ ì„ë² ë”© í¬ê¸° í™•ì¸"""
            if state.global_step % 1000 == 0 and model is not None:
                if model.get_input_embeddings().weight.shape[0] != len(self.tokenizer):
                    logger.warning(f"ì„ë² ë”© í¬ê¸° ë¶ˆì¼ì¹˜ ê°ì§€: {model.get_input_embeddings().weight.shape[0]} vs {len(self.tokenizer)}")
                    model.resize_token_embeddings(len(self.tokenizer))
        
        def on_load_checkpoint(self, args, state, control, **kwargs):
            """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œ í˜¸ì¶œ"""
            logger.info("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘...")
            
        def on_checkpoint_model_loading(self, args, state, control, model=None, **kwargs):
            """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œ í˜¸ì¶œ"""
            if model is not None:
                # ì„ë² ë”© í¬ê¸° ì¡°ì •
                model.resize_token_embeddings(len(self.tokenizer))
                logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í›„ ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì •: {len(self.tokenizer)}")
                
                # ìƒì„± ì„¤ì • ë‹¤ì‹œ ì ìš©
                model.generation_config = generation_config
                
                # ë¯¸ë¦¬ êµ¬ì„±ëœ ëª¨ë¸ë¡œ ì´ˆê¸° ê°€ì¤‘ì¹˜ ë³µì‚¬ (ì„ íƒ ì‚¬í•­)
                if hasattr(model, 'lm_head') and not hasattr(model.lm_head, 'weight'):
                    logger.warning("lm_headì— weightê°€ ì—†ìŒ, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í•„ìš”")
                    # í•„ìš”í•œ ê²½ìš° ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë¡œì§ ì¶”ê°€

    # íŠ¹ìˆ˜ í† í° ID ëª©ë¡
    special_token_ids = [
        tokenizer.bos_token_id, 
        tokenizer.eos_token_id, 
        tokenizer.pad_token_id,
        *[tokenizer.convert_tokens_to_ids(t) for t in SPECIAL_TOKENS.values()]
    ]
    
    # ìµœì¢… ì½œë°± ëª©ë¡ ìƒì„±
    callbacks = [
        EarlyStoppingCallback(
            early_stopping_patience=args.patience,
            early_stopping_threshold=0.0
        ),
        TokenEmbeddingCallback(tokenizer, special_token_ids)
    ]

    # ------------------------------------------------------------------
    # Custom Trainer: ë©€í‹°íƒœìŠ¤í¬(ì „ëµ ë¶„ë¥˜ + ì‘ë‹µ ìƒì„±) + ì˜ˆì¸¡ ì „ëµ prefix ìƒì„±
    # ------------------------------------------------------------------
    from transformers.trainer_utils import PredictionOutput
    import torch.nn.functional as F

    class MultiTaskPrefixTrainer(Seq2SeqTrainer):
        """generation loss + alpha * strategy classification loss"""

        def __init__(self, *trainer_args, strategy_head=None, tokenizer=None, alpha=0.5, **trainer_kwargs):
            super().__init__(*trainer_args, **trainer_kwargs)
            self.strategy_head = strategy_head
            self._tokenizer = tokenizer
            self.alpha = alpha

        # --------- loss ê³„ì‚° (train) ---------
        def compute_loss(self, model, inputs, return_outputs=False):  # type: ignore
            # strategy label ë¶„ë¦¬
            strat_labels = inputs.pop("strategy_id")

            # generation loss (Bart ë‚´ì¥)
            outputs = model(**inputs)
            gen_loss = outputs.loss

            # encoder CLS vector â†’ strategy logits
            cls_vec = outputs.encoder_last_hidden_state[:, 0]
            # ensure head device ì¼ì¹˜
            if self.strategy_head.weight.device != cls_vec.device:
                self.strategy_head = self.strategy_head.to(cls_vec.device)
            strat_logits = self.strategy_head(cls_vec)
            strat_loss = F.cross_entropy(strat_logits, strat_labels)

            total_loss = gen_loss + self.alpha * strat_loss

            return (total_loss, outputs) if return_outputs else total_loss

        # --------- prediction (eval / predict) ---------
        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):  # type: ignore
            has_labels = "labels" in inputs
            inputs = self._prepare_inputs(inputs)

            # â¤ optional loss ê³„ì‚°
            loss = None
            if has_labels:
                with torch.no_grad():
                    outputs = model(**inputs)
                    loss = outputs.loss.detach()

            # â¤ strategy prediction
            with torch.no_grad():
                enc_out = model.model.encoder(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask"),
                    return_dict=True,
                )
                cls_vec = enc_out.last_hidden_state[:, 0]
                if self.strategy_head.weight.device != cls_vec.device:
                    self.strategy_head = self.strategy_head.to(cls_vec.device)
                logits = self.strategy_head(cls_vec)
                strat_ids = torch.argmax(logits, dim=-1)  # (B,)

            # ì „ëµ í† í° id ë°°ì¹˜
            strat_tok_ids = [
                self._tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[STRATEGIES[i]])
                for i in strat_ids.tolist()
            ]
            strat_tok_batch = torch.tensor(strat_tok_ids, device=model.device).unsqueeze(1)  # (B,1)

            # decoder_input_ids = <s> + strategy_token
            bos_col = torch.full_like(strat_tok_batch, model.config.decoder_start_token_id)
            decoder_input_ids = torch.cat([bos_col, strat_tok_batch], dim=1)

            generated_tokens = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask"),
                decoder_input_ids=decoder_input_ids,
                max_length=self.args.generation_max_length or model.generation_config.max_length,
            )

            if prediction_loss_only:
                return (loss, None, None)

            labels = inputs["labels"] if has_labels else None

            # evaluation_loop expects (losses, preds, labels)
            if loss is not None:
                batch_size = inputs["input_ids"].shape[0]
                loss = loss.repeat(batch_size)

            return (loss, generated_tokens, labels)


    trainer = MultiTaskPrefixTrainer(
        model=model,
        args=t_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=callbacks,
        strategy_head=strategy_head,
        tokenizer=tokenizer,
        alpha=args.alpha,
    )

    # --------------------- ì´ˆê¸° ëª¨ë¸(epoch 0) í‰ê°€ ---------------------
    if args.eval_init or args.eval_only:
        logger.info("ğŸ“Š Evaluating initial model (epoch 0) ...")
        
        # ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€
        init_eval = trainer.evaluate()
        
        # ê²°ê³¼ ì €ì¥
        init_path = Path(args.output_dir) / "init_eval_metrics.json"
        init_path.parent.mkdir(exist_ok=True, parents=True)
        with init_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) if isinstance(v, (int, float, np.number)) else v 
                       for k, v in init_eval.items()}, f, indent=2)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€
        logger.info("ğŸ“Š Evaluating initial model on test split ...")
        
        
        # í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
        test_out = trainer.predict(test_ds, metric_key_prefix="init_test")
        
        # PPL ê³„ì‚°
        test_ppl = float(np.exp(test_out.metrics.get('init_test_loss', 0)))
        logger.info(f"Initial Test Perplexity: {test_ppl:.4f}")
        
        # ìƒ˜í”Œ ì €ì¥ìš© ì›ë³¸ í…ìŠ¤íŠ¸ (MultiESC ì „ì²˜ë¦¬ ì „)
        lbl_ids = test_out.label_ids.copy()
        lbl_ids[lbl_ids == -100] = tokenizer.pad_token_id
        
        gen_raw = tokenizer.batch_decode(test_out.predictions, skip_special_tokens=True)
        ref_raw = tokenizer.batch_decode(lbl_ids, skip_special_tokens=True)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°ìš© í…ìŠ¤íŠ¸ (MultiESC ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬)
        gen_txt = []
        ref_txt = []
        for gen, ref in zip(gen_raw, ref_raw):
            gen_processed = ' '.join(nltk.word_tokenize(gen.lower()))
            ref_processed = ' '.join(nltk.word_tokenize(ref.lower()))
            gen_txt.append(gen_processed)
            ref_txt.append(ref_processed)
            
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        text_metrics = generation_metrics(gen_txt, ref_txt)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        init_test_metrics = {}
        for k, v in text_metrics.items():
            init_test_metrics[f'init_test_{k}'] = v
        init_test_metrics.update(test_out.metrics)
        init_test_metrics['init_test_perplexity'] = test_ppl
        
        # ê²°ê³¼ ì €ì¥
        init_test_path = Path(args.output_dir) / "init_test_metrics.json"
        with init_test_path.open("w", encoding="utf-8") as f:
            json.dump({k: float(v) if isinstance(v, (int, float, np.number)) else v 
                       for k, v in init_test_metrics.items()}, f, indent=2)
        
        # ìƒ˜í”Œ ì €ì¥ (ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ì €ì¥)
        sample_n = min(10, len(gen_raw))
        with open(Path(args.output_dir) / "init_samples.txt", "w", encoding="utf-8") as f:
            for i, (ref, gen) in enumerate(zip(ref_raw[:sample_n], gen_raw[:sample_n])):
                context = test_ds.examples[i]["context"]
                f.write(f"CONTEXT: {context}\nREF: {ref}\nGEN: {gen}\n---\n")
        
        logger.info(f"ğŸ“ Saved initial model metrics to {args.output_dir}")
    
    # í•™ìŠµ ìˆ˜í–‰ (eval_onlyê°€ Trueë©´ í•™ìŠµ ê±´ë„ˆëœ€)
    if not args.eval_only:
        trainer.train()
        
        # í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥ - ì¤‘ìš”: ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ì €ì¥
        # safe_serialization=TrueëŠ” ë¯¸ë˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì˜µì…˜
        model_path = Path(args.output_dir)
        model.save_pretrained(model_path, safe_serialization=True)
        tokenizer.save_pretrained(model_path)
        
        logger.info(f"Model explicitly saved to {args.output_dir}")
        
        # --------------------- test split í‰ê°€ ---------------------
        logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ ì¤‘...")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
        test_out = trainer.predict(test_ds, metric_key_prefix="test")

        # PPL ê³„ì‚°
        test_ppl = float(np.exp(test_out.metrics.get('test_loss', 0)))
        logger.info(f"Test Perplexity: {test_ppl:.4f}")

        # ìƒ˜í”Œ ì €ì¥ìš© ì›ë³¸ í…ìŠ¤íŠ¸ (MultiESC ì „ì²˜ë¦¬ ì „)
        lbl_ids = test_out.label_ids.copy()
        lbl_ids[lbl_ids == -100] = tokenizer.pad_token_id
        
        gen_raw = tokenizer.batch_decode(test_out.predictions, skip_special_tokens=True)
        ref_raw = tokenizer.batch_decode(lbl_ids, skip_special_tokens=True)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°ìš© í…ìŠ¤íŠ¸ (MultiESC ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬)
        gen_txt = []
        ref_txt = []
        for gen, ref in zip(gen_raw, ref_raw):
            gen_processed = ' '.join(nltk.word_tokenize(gen.lower()))
            ref_processed = ' '.join(nltk.word_tokenize(ref.lower()))
            gen_txt.append(gen_processed)
            ref_txt.append(ref_processed)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        text_metrics = generation_metrics(gen_txt, ref_txt)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        test_metrics = {}
        for k, v in text_metrics.items():
            test_metrics[f'test_{k}'] = v
        test_metrics.update(test_out.metrics)
        test_metrics['test_perplexity'] = test_ppl
        
        with open(Path(args.output_dir) / "test_metrics.json", "w") as f:
            json.dump({k: float(v) if isinstance(v, (int, float, np.number)) else v 
                       for k, v in test_metrics.items()}, f, indent=2)

        # ìƒ˜í”Œ ì €ì¥ (ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ì €ì¥)
        sample_n = min(20, len(gen_raw))
        with open(Path(args.output_dir) / "test_samples.txt", "w", encoding="utf-8") as f:
            for i, (ref, gen) in enumerate(zip(ref_raw[:sample_n], gen_raw[:sample_n])):
                context = test_ds.examples[i]["context"]
                f.write(f"CONTEXT: {context}\nREF: {ref}\nGEN: {gen}\n---\n")

        logger.info(f"ëª¨ë¸ ë° í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì €ì¥ ì™„ë£Œ: {args.output_dir}")

    if args.show_samples:
        import textwrap
        # í† í¬ë‚˜ì´ì €ë¡œ í™•ì¸í•˜ëŠ” ìƒ˜í”Œ ë°ì´í„°
        for i in random.sample(range(len(train_ds)), args.show_samples):
            ex = train_ds[i]

            ctx_plain = safe_decode(ex["input_ids"], tokenizer, skip_special_tokens=False)
            tgt_plain = safe_decode(ex["labels"], tokenizer, skip_special_tokens=False)
            
            logging.info(
                "\n===== SAMPLE {:d} =====\n"
                "CONTEXT:\n{}\n\n"
                "TARGET:\n{}\n"
                "{}".format(
                    i,
                    ctx_plain, 
                    tgt_plain,
                    "=" * 60
                )
            )
        
        # ì‹¤ì œ ìƒì„± í…ŒìŠ¤íŠ¸
        logger.info("\n===== ìƒì„± ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ =====")
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
        model.eval()
        
        # ëª‡ ê°œì˜ ëœë¤ ìƒ˜í”Œë¡œ ìƒì„± í…ŒìŠ¤íŠ¸
        for i in random.sample(range(len(val_ds)), min(3, len(val_ds))):
            ex = val_ds[i]
            input_ids = torch.tensor([ex["input_ids"]]).to(model.device)
            attention_mask = torch.tensor([ex["attention_mask"]]).to(model.device)
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=args.max_tgt_length,
                )
            
            # ë””ì½”ë”© ë° ì¶œë ¥ - pad í† í°ë§Œ ì œê±°í•˜ê³  íŠ¹ìˆ˜ í† í°ì€ ìœ ì§€
            skip_special_tokens = False

            generated_text = safe_decode(outputs[0].tolist(), tokenizer, skip_special_tokens=skip_special_tokens)
            target_text = safe_decode(ex["labels"], tokenizer, skip_special_tokens=skip_special_tokens)
            context_text = safe_decode(ex["input_ids"], tokenizer, skip_special_tokens=skip_special_tokens)
            
            logger.info(
                "\n----- ìƒì„± ìƒ˜í”Œ {:d} -----\n"
                "ì…ë ¥ ë¬¸ë§¥:\n{}\n\n"
                "ì •ë‹µ ì‘ë‹µ:\n{}\n\n"
                "ìƒì„± ì‘ë‹µ:\n{}\n"
                "{}".format(
                    i,
                    context_text,  
                    target_text,   
                    generated_text,
                    "-" * 60
                )
            )
        
        return


if __name__ == "__main__":
    main() 